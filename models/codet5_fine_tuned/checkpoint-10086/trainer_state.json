{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 10086,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00297441998810232,
      "grad_norm": 12.206463813781738,
      "learning_rate": 1.9982153480071388e-05,
      "loss": 6.3429,
      "step": 10
    },
    {
      "epoch": 0.00594883997620464,
      "grad_norm": 7.168834209442139,
      "learning_rate": 1.996232401348404e-05,
      "loss": 4.2192,
      "step": 20
    },
    {
      "epoch": 0.00892325996430696,
      "grad_norm": 5.448003768920898,
      "learning_rate": 1.994249454689669e-05,
      "loss": 3.5702,
      "step": 30
    },
    {
      "epoch": 0.01189767995240928,
      "grad_norm": 5.261133670806885,
      "learning_rate": 1.992266508030934e-05,
      "loss": 3.149,
      "step": 40
    },
    {
      "epoch": 0.0148720999405116,
      "grad_norm": 12.68238639831543,
      "learning_rate": 1.9902835613721992e-05,
      "loss": 3.0285,
      "step": 50
    },
    {
      "epoch": 0.01784651992861392,
      "grad_norm": 4.837400913238525,
      "learning_rate": 1.9883006147134643e-05,
      "loss": 2.7546,
      "step": 60
    },
    {
      "epoch": 0.02082093991671624,
      "grad_norm": 8.377693176269531,
      "learning_rate": 1.9863176680547294e-05,
      "loss": 2.7802,
      "step": 70
    },
    {
      "epoch": 0.02379535990481856,
      "grad_norm": 5.494658946990967,
      "learning_rate": 1.984334721395995e-05,
      "loss": 2.5857,
      "step": 80
    },
    {
      "epoch": 0.02676977989292088,
      "grad_norm": 7.342787742614746,
      "learning_rate": 1.98235177473726e-05,
      "loss": 2.8014,
      "step": 90
    },
    {
      "epoch": 0.0297441998810232,
      "grad_norm": 4.140503406524658,
      "learning_rate": 1.980368828078525e-05,
      "loss": 2.586,
      "step": 100
    },
    {
      "epoch": 0.03271861986912552,
      "grad_norm": 4.6177239418029785,
      "learning_rate": 1.9783858814197902e-05,
      "loss": 2.251,
      "step": 110
    },
    {
      "epoch": 0.03569303985722784,
      "grad_norm": 3.4244468212127686,
      "learning_rate": 1.9764029347610553e-05,
      "loss": 2.2034,
      "step": 120
    },
    {
      "epoch": 0.03866745984533016,
      "grad_norm": 6.052887439727783,
      "learning_rate": 1.9744199881023204e-05,
      "loss": 2.1189,
      "step": 130
    },
    {
      "epoch": 0.04164187983343248,
      "grad_norm": 11.615580558776855,
      "learning_rate": 1.9724370414435855e-05,
      "loss": 2.1405,
      "step": 140
    },
    {
      "epoch": 0.044616299821534804,
      "grad_norm": 3.490060567855835,
      "learning_rate": 1.9704540947848506e-05,
      "loss": 2.0832,
      "step": 150
    },
    {
      "epoch": 0.04759071980963712,
      "grad_norm": 3.5977296829223633,
      "learning_rate": 1.9684711481261157e-05,
      "loss": 1.8427,
      "step": 160
    },
    {
      "epoch": 0.05056513979773944,
      "grad_norm": 24.23368263244629,
      "learning_rate": 1.966488201467381e-05,
      "loss": 1.9177,
      "step": 170
    },
    {
      "epoch": 0.05353955978584176,
      "grad_norm": 3.393515110015869,
      "learning_rate": 1.964505254808646e-05,
      "loss": 1.9328,
      "step": 180
    },
    {
      "epoch": 0.05651397977394408,
      "grad_norm": 2.5767576694488525,
      "learning_rate": 1.962522308149911e-05,
      "loss": 1.8874,
      "step": 190
    },
    {
      "epoch": 0.0594883997620464,
      "grad_norm": 3.2030701637268066,
      "learning_rate": 1.9605393614911762e-05,
      "loss": 1.8264,
      "step": 200
    },
    {
      "epoch": 0.062462819750148724,
      "grad_norm": 21.687602996826172,
      "learning_rate": 1.9585564148324413e-05,
      "loss": 1.6833,
      "step": 210
    },
    {
      "epoch": 0.06543723973825104,
      "grad_norm": 92.59425354003906,
      "learning_rate": 1.9565734681737064e-05,
      "loss": 1.7964,
      "step": 220
    },
    {
      "epoch": 0.06841165972635337,
      "grad_norm": 3.554387092590332,
      "learning_rate": 1.9545905215149715e-05,
      "loss": 1.6473,
      "step": 230
    },
    {
      "epoch": 0.07138607971445568,
      "grad_norm": 3.5597403049468994,
      "learning_rate": 1.9526075748562366e-05,
      "loss": 1.8096,
      "step": 240
    },
    {
      "epoch": 0.074360499702558,
      "grad_norm": 3.114166736602783,
      "learning_rate": 1.9506246281975017e-05,
      "loss": 1.9668,
      "step": 250
    },
    {
      "epoch": 0.07733491969066032,
      "grad_norm": 3.406670093536377,
      "learning_rate": 1.948641681538767e-05,
      "loss": 1.7607,
      "step": 260
    },
    {
      "epoch": 0.08030933967876264,
      "grad_norm": 2.8590478897094727,
      "learning_rate": 1.946658734880032e-05,
      "loss": 1.7085,
      "step": 270
    },
    {
      "epoch": 0.08328375966686496,
      "grad_norm": 3.0122222900390625,
      "learning_rate": 1.944675788221297e-05,
      "loss": 1.5894,
      "step": 280
    },
    {
      "epoch": 0.08625817965496728,
      "grad_norm": 3.4865615367889404,
      "learning_rate": 1.942692841562562e-05,
      "loss": 1.6616,
      "step": 290
    },
    {
      "epoch": 0.08923259964306961,
      "grad_norm": 3.2718722820281982,
      "learning_rate": 1.9407098949038273e-05,
      "loss": 1.6396,
      "step": 300
    },
    {
      "epoch": 0.09220701963117192,
      "grad_norm": 3.4432222843170166,
      "learning_rate": 1.9387269482450924e-05,
      "loss": 1.774,
      "step": 310
    },
    {
      "epoch": 0.09518143961927424,
      "grad_norm": 3.624422073364258,
      "learning_rate": 1.9367440015863575e-05,
      "loss": 1.6782,
      "step": 320
    },
    {
      "epoch": 0.09815585960737656,
      "grad_norm": 2.974832773208618,
      "learning_rate": 1.9347610549276226e-05,
      "loss": 1.7056,
      "step": 330
    },
    {
      "epoch": 0.10113027959547888,
      "grad_norm": 2.678624153137207,
      "learning_rate": 1.9327781082688877e-05,
      "loss": 1.4924,
      "step": 340
    },
    {
      "epoch": 0.1041046995835812,
      "grad_norm": 3.1129393577575684,
      "learning_rate": 1.9307951616101528e-05,
      "loss": 1.626,
      "step": 350
    },
    {
      "epoch": 0.10707911957168352,
      "grad_norm": 4.675744533538818,
      "learning_rate": 1.928812214951418e-05,
      "loss": 1.5796,
      "step": 360
    },
    {
      "epoch": 0.11005353955978585,
      "grad_norm": 2.571082353591919,
      "learning_rate": 1.926829268292683e-05,
      "loss": 1.4948,
      "step": 370
    },
    {
      "epoch": 0.11302795954788816,
      "grad_norm": 2.68765926361084,
      "learning_rate": 1.924846321633948e-05,
      "loss": 1.6048,
      "step": 380
    },
    {
      "epoch": 0.11600237953599048,
      "grad_norm": 6.730815410614014,
      "learning_rate": 1.9228633749752133e-05,
      "loss": 1.4702,
      "step": 390
    },
    {
      "epoch": 0.1189767995240928,
      "grad_norm": 5.075377464294434,
      "learning_rate": 1.9208804283164784e-05,
      "loss": 1.5326,
      "step": 400
    },
    {
      "epoch": 0.12195121951219512,
      "grad_norm": 5.856057167053223,
      "learning_rate": 1.9188974816577435e-05,
      "loss": 1.5531,
      "step": 410
    },
    {
      "epoch": 0.12492563950029745,
      "grad_norm": 3.6947968006134033,
      "learning_rate": 1.9169145349990086e-05,
      "loss": 1.5751,
      "step": 420
    },
    {
      "epoch": 0.12790005948839978,
      "grad_norm": 10.230768203735352,
      "learning_rate": 1.9149315883402737e-05,
      "loss": 1.6552,
      "step": 430
    },
    {
      "epoch": 0.13087447947650208,
      "grad_norm": 3.0271806716918945,
      "learning_rate": 1.9129486416815388e-05,
      "loss": 1.4748,
      "step": 440
    },
    {
      "epoch": 0.1338488994646044,
      "grad_norm": 2.843221664428711,
      "learning_rate": 1.910965695022804e-05,
      "loss": 1.4845,
      "step": 450
    },
    {
      "epoch": 0.13682331945270673,
      "grad_norm": 3.230980634689331,
      "learning_rate": 1.9089827483640694e-05,
      "loss": 1.5451,
      "step": 460
    },
    {
      "epoch": 0.13979773944080903,
      "grad_norm": 3.8489248752593994,
      "learning_rate": 1.9069998017053345e-05,
      "loss": 1.5589,
      "step": 470
    },
    {
      "epoch": 0.14277215942891136,
      "grad_norm": 3.0409021377563477,
      "learning_rate": 1.9050168550465996e-05,
      "loss": 1.6614,
      "step": 480
    },
    {
      "epoch": 0.1457465794170137,
      "grad_norm": 3.8526597023010254,
      "learning_rate": 1.9030339083878647e-05,
      "loss": 1.4857,
      "step": 490
    },
    {
      "epoch": 0.148720999405116,
      "grad_norm": 6.855503559112549,
      "learning_rate": 1.9010509617291298e-05,
      "loss": 1.6255,
      "step": 500
    },
    {
      "epoch": 0.15169541939321832,
      "grad_norm": 3.152487277984619,
      "learning_rate": 1.899068015070395e-05,
      "loss": 1.4668,
      "step": 510
    },
    {
      "epoch": 0.15466983938132065,
      "grad_norm": 3.1714038848876953,
      "learning_rate": 1.89708506841166e-05,
      "loss": 1.4655,
      "step": 520
    },
    {
      "epoch": 0.15764425936942297,
      "grad_norm": 3.4940173625946045,
      "learning_rate": 1.895102121752925e-05,
      "loss": 1.417,
      "step": 530
    },
    {
      "epoch": 0.16061867935752527,
      "grad_norm": 2.9947922229766846,
      "learning_rate": 1.8931191750941902e-05,
      "loss": 1.3571,
      "step": 540
    },
    {
      "epoch": 0.1635930993456276,
      "grad_norm": 3.883592128753662,
      "learning_rate": 1.8911362284354553e-05,
      "loss": 1.5564,
      "step": 550
    },
    {
      "epoch": 0.16656751933372993,
      "grad_norm": 2.8521807193756104,
      "learning_rate": 1.8891532817767205e-05,
      "loss": 1.5841,
      "step": 560
    },
    {
      "epoch": 0.16954193932183223,
      "grad_norm": 4.08410120010376,
      "learning_rate": 1.8871703351179856e-05,
      "loss": 1.3311,
      "step": 570
    },
    {
      "epoch": 0.17251635930993456,
      "grad_norm": 60.513816833496094,
      "learning_rate": 1.8851873884592507e-05,
      "loss": 1.3733,
      "step": 580
    },
    {
      "epoch": 0.1754907792980369,
      "grad_norm": 3.6013875007629395,
      "learning_rate": 1.8832044418005158e-05,
      "loss": 1.4295,
      "step": 590
    },
    {
      "epoch": 0.17846519928613921,
      "grad_norm": 3.239874839782715,
      "learning_rate": 1.881221495141781e-05,
      "loss": 1.3887,
      "step": 600
    },
    {
      "epoch": 0.18143961927424151,
      "grad_norm": 3.127769708633423,
      "learning_rate": 1.879238548483046e-05,
      "loss": 1.3849,
      "step": 610
    },
    {
      "epoch": 0.18441403926234384,
      "grad_norm": 3.2768428325653076,
      "learning_rate": 1.877255601824311e-05,
      "loss": 1.362,
      "step": 620
    },
    {
      "epoch": 0.18738845925044617,
      "grad_norm": 3.4373481273651123,
      "learning_rate": 1.8752726551655762e-05,
      "loss": 1.3531,
      "step": 630
    },
    {
      "epoch": 0.19036287923854847,
      "grad_norm": 2.2812340259552,
      "learning_rate": 1.8732897085068413e-05,
      "loss": 1.2687,
      "step": 640
    },
    {
      "epoch": 0.1933372992266508,
      "grad_norm": 3.229750633239746,
      "learning_rate": 1.8713067618481064e-05,
      "loss": 1.3152,
      "step": 650
    },
    {
      "epoch": 0.19631171921475313,
      "grad_norm": 5.853758811950684,
      "learning_rate": 1.8693238151893716e-05,
      "loss": 1.5333,
      "step": 660
    },
    {
      "epoch": 0.19928613920285546,
      "grad_norm": 3.0973949432373047,
      "learning_rate": 1.8673408685306367e-05,
      "loss": 1.3922,
      "step": 670
    },
    {
      "epoch": 0.20226055919095776,
      "grad_norm": 6.991563320159912,
      "learning_rate": 1.8653579218719018e-05,
      "loss": 1.44,
      "step": 680
    },
    {
      "epoch": 0.20523497917906008,
      "grad_norm": 4.4147443771362305,
      "learning_rate": 1.863374975213167e-05,
      "loss": 1.3302,
      "step": 690
    },
    {
      "epoch": 0.2082093991671624,
      "grad_norm": 3.841146469116211,
      "learning_rate": 1.861392028554432e-05,
      "loss": 1.2294,
      "step": 700
    },
    {
      "epoch": 0.2111838191552647,
      "grad_norm": 2.464505672454834,
      "learning_rate": 1.859409081895697e-05,
      "loss": 1.2491,
      "step": 710
    },
    {
      "epoch": 0.21415823914336704,
      "grad_norm": 2.807264804840088,
      "learning_rate": 1.8574261352369622e-05,
      "loss": 1.2532,
      "step": 720
    },
    {
      "epoch": 0.21713265913146937,
      "grad_norm": 2.2039854526519775,
      "learning_rate": 1.8554431885782273e-05,
      "loss": 1.3264,
      "step": 730
    },
    {
      "epoch": 0.2201070791195717,
      "grad_norm": 3.0495564937591553,
      "learning_rate": 1.8534602419194924e-05,
      "loss": 1.3448,
      "step": 740
    },
    {
      "epoch": 0.223081499107674,
      "grad_norm": 2.85453200340271,
      "learning_rate": 1.8514772952607575e-05,
      "loss": 1.3347,
      "step": 750
    },
    {
      "epoch": 0.22605591909577633,
      "grad_norm": 3.0584769248962402,
      "learning_rate": 1.8494943486020226e-05,
      "loss": 1.2722,
      "step": 760
    },
    {
      "epoch": 0.22903033908387865,
      "grad_norm": 3.6269588470458984,
      "learning_rate": 1.8475114019432878e-05,
      "loss": 1.4278,
      "step": 770
    },
    {
      "epoch": 0.23200475907198095,
      "grad_norm": 13.886163711547852,
      "learning_rate": 1.845528455284553e-05,
      "loss": 1.4396,
      "step": 780
    },
    {
      "epoch": 0.23497917906008328,
      "grad_norm": 2.4580461978912354,
      "learning_rate": 1.843545508625818e-05,
      "loss": 1.2254,
      "step": 790
    },
    {
      "epoch": 0.2379535990481856,
      "grad_norm": 2.317307472229004,
      "learning_rate": 1.841562561967083e-05,
      "loss": 1.2925,
      "step": 800
    },
    {
      "epoch": 0.2409280190362879,
      "grad_norm": 3.217766284942627,
      "learning_rate": 1.8395796153083482e-05,
      "loss": 1.3919,
      "step": 810
    },
    {
      "epoch": 0.24390243902439024,
      "grad_norm": 2.3793962001800537,
      "learning_rate": 1.8375966686496133e-05,
      "loss": 1.2836,
      "step": 820
    },
    {
      "epoch": 0.24687685901249257,
      "grad_norm": 4.050698280334473,
      "learning_rate": 1.8356137219908784e-05,
      "loss": 1.4544,
      "step": 830
    },
    {
      "epoch": 0.2498512790005949,
      "grad_norm": 2.664412498474121,
      "learning_rate": 1.8336307753321435e-05,
      "loss": 1.3509,
      "step": 840
    },
    {
      "epoch": 0.2528256989886972,
      "grad_norm": 2.7599892616271973,
      "learning_rate": 1.8316478286734086e-05,
      "loss": 1.5338,
      "step": 850
    },
    {
      "epoch": 0.25580011897679955,
      "grad_norm": 2.4582622051239014,
      "learning_rate": 1.8296648820146737e-05,
      "loss": 1.1725,
      "step": 860
    },
    {
      "epoch": 0.2587745389649018,
      "grad_norm": 2.9952173233032227,
      "learning_rate": 1.8276819353559392e-05,
      "loss": 1.1746,
      "step": 870
    },
    {
      "epoch": 0.26174895895300415,
      "grad_norm": 4.27047872543335,
      "learning_rate": 1.8256989886972043e-05,
      "loss": 1.2628,
      "step": 880
    },
    {
      "epoch": 0.2647233789411065,
      "grad_norm": 3.159250020980835,
      "learning_rate": 1.8237160420384694e-05,
      "loss": 1.3829,
      "step": 890
    },
    {
      "epoch": 0.2676977989292088,
      "grad_norm": 2.5855700969696045,
      "learning_rate": 1.8217330953797345e-05,
      "loss": 1.2958,
      "step": 900
    },
    {
      "epoch": 0.27067221891731114,
      "grad_norm": 2.9018449783325195,
      "learning_rate": 1.8197501487209996e-05,
      "loss": 1.3902,
      "step": 910
    },
    {
      "epoch": 0.27364663890541346,
      "grad_norm": 2.6354081630706787,
      "learning_rate": 1.8177672020622647e-05,
      "loss": 1.3352,
      "step": 920
    },
    {
      "epoch": 0.2766210588935158,
      "grad_norm": 2.684629201889038,
      "learning_rate": 1.81578425540353e-05,
      "loss": 1.3423,
      "step": 930
    },
    {
      "epoch": 0.27959547888161806,
      "grad_norm": 6.150338172912598,
      "learning_rate": 1.813801308744795e-05,
      "loss": 1.3492,
      "step": 940
    },
    {
      "epoch": 0.2825698988697204,
      "grad_norm": 2.445648193359375,
      "learning_rate": 1.81181836208606e-05,
      "loss": 1.2896,
      "step": 950
    },
    {
      "epoch": 0.2855443188578227,
      "grad_norm": 4.8894453048706055,
      "learning_rate": 1.8098354154273252e-05,
      "loss": 1.4536,
      "step": 960
    },
    {
      "epoch": 0.28851873884592505,
      "grad_norm": 2.473299026489258,
      "learning_rate": 1.8078524687685903e-05,
      "loss": 1.3643,
      "step": 970
    },
    {
      "epoch": 0.2914931588340274,
      "grad_norm": 23.638202667236328,
      "learning_rate": 1.8058695221098554e-05,
      "loss": 1.2645,
      "step": 980
    },
    {
      "epoch": 0.2944675788221297,
      "grad_norm": 6.65744686126709,
      "learning_rate": 1.8038865754511205e-05,
      "loss": 1.1249,
      "step": 990
    },
    {
      "epoch": 0.297441998810232,
      "grad_norm": 11.325952529907227,
      "learning_rate": 1.8019036287923856e-05,
      "loss": 1.2819,
      "step": 1000
    },
    {
      "epoch": 0.3004164187983343,
      "grad_norm": 2.921778678894043,
      "learning_rate": 1.7999206821336507e-05,
      "loss": 1.2455,
      "step": 1010
    },
    {
      "epoch": 0.30339083878643663,
      "grad_norm": 2.331556797027588,
      "learning_rate": 1.7979377354749158e-05,
      "loss": 1.1537,
      "step": 1020
    },
    {
      "epoch": 0.30636525877453896,
      "grad_norm": 2.4061410427093506,
      "learning_rate": 1.795954788816181e-05,
      "loss": 1.1857,
      "step": 1030
    },
    {
      "epoch": 0.3093396787626413,
      "grad_norm": 5.299426078796387,
      "learning_rate": 1.793971842157446e-05,
      "loss": 1.3352,
      "step": 1040
    },
    {
      "epoch": 0.3123140987507436,
      "grad_norm": 3.9725236892700195,
      "learning_rate": 1.791988895498711e-05,
      "loss": 1.2124,
      "step": 1050
    },
    {
      "epoch": 0.31528851873884595,
      "grad_norm": 2.269361734390259,
      "learning_rate": 1.7900059488399763e-05,
      "loss": 1.2539,
      "step": 1060
    },
    {
      "epoch": 0.3182629387269482,
      "grad_norm": 4.320427894592285,
      "learning_rate": 1.7880230021812414e-05,
      "loss": 1.3849,
      "step": 1070
    },
    {
      "epoch": 0.32123735871505055,
      "grad_norm": 2.3579134941101074,
      "learning_rate": 1.7860400555225065e-05,
      "loss": 1.1339,
      "step": 1080
    },
    {
      "epoch": 0.3242117787031529,
      "grad_norm": 7.163400650024414,
      "learning_rate": 1.7840571088637716e-05,
      "loss": 1.284,
      "step": 1090
    },
    {
      "epoch": 0.3271861986912552,
      "grad_norm": 2.169250965118408,
      "learning_rate": 1.7820741622050367e-05,
      "loss": 1.3284,
      "step": 1100
    },
    {
      "epoch": 0.33016061867935753,
      "grad_norm": 2.8946893215179443,
      "learning_rate": 1.7800912155463018e-05,
      "loss": 1.293,
      "step": 1110
    },
    {
      "epoch": 0.33313503866745986,
      "grad_norm": 2.713425874710083,
      "learning_rate": 1.778108268887567e-05,
      "loss": 1.2109,
      "step": 1120
    },
    {
      "epoch": 0.3361094586555622,
      "grad_norm": 2.674452781677246,
      "learning_rate": 1.776125322228832e-05,
      "loss": 1.2702,
      "step": 1130
    },
    {
      "epoch": 0.33908387864366446,
      "grad_norm": 10.290002822875977,
      "learning_rate": 1.774142375570097e-05,
      "loss": 1.2372,
      "step": 1140
    },
    {
      "epoch": 0.3420582986317668,
      "grad_norm": 9.781719207763672,
      "learning_rate": 1.7721594289113622e-05,
      "loss": 1.0801,
      "step": 1150
    },
    {
      "epoch": 0.3450327186198691,
      "grad_norm": 5.25590705871582,
      "learning_rate": 1.7701764822526274e-05,
      "loss": 1.2395,
      "step": 1160
    },
    {
      "epoch": 0.34800713860797144,
      "grad_norm": 3.373473882675171,
      "learning_rate": 1.7681935355938925e-05,
      "loss": 1.3536,
      "step": 1170
    },
    {
      "epoch": 0.3509815585960738,
      "grad_norm": 1.9393346309661865,
      "learning_rate": 1.7662105889351576e-05,
      "loss": 1.1369,
      "step": 1180
    },
    {
      "epoch": 0.3539559785841761,
      "grad_norm": 3.0183703899383545,
      "learning_rate": 1.7642276422764227e-05,
      "loss": 1.1659,
      "step": 1190
    },
    {
      "epoch": 0.35693039857227843,
      "grad_norm": 2.599658489227295,
      "learning_rate": 1.7622446956176878e-05,
      "loss": 1.1717,
      "step": 1200
    },
    {
      "epoch": 0.3599048185603807,
      "grad_norm": 2.9903337955474854,
      "learning_rate": 1.7602617489589532e-05,
      "loss": 1.1804,
      "step": 1210
    },
    {
      "epoch": 0.36287923854848303,
      "grad_norm": 2.7006661891937256,
      "learning_rate": 1.7582788023002184e-05,
      "loss": 1.1777,
      "step": 1220
    },
    {
      "epoch": 0.36585365853658536,
      "grad_norm": 2.9031214714050293,
      "learning_rate": 1.7562958556414835e-05,
      "loss": 1.1531,
      "step": 1230
    },
    {
      "epoch": 0.3688280785246877,
      "grad_norm": 2.9736328125,
      "learning_rate": 1.7543129089827486e-05,
      "loss": 1.5126,
      "step": 1240
    },
    {
      "epoch": 0.37180249851279,
      "grad_norm": 4.4943766593933105,
      "learning_rate": 1.7523299623240137e-05,
      "loss": 1.366,
      "step": 1250
    },
    {
      "epoch": 0.37477691850089234,
      "grad_norm": 3.0322461128234863,
      "learning_rate": 1.7503470156652788e-05,
      "loss": 1.2457,
      "step": 1260
    },
    {
      "epoch": 0.37775133848899467,
      "grad_norm": 4.542895793914795,
      "learning_rate": 1.748364069006544e-05,
      "loss": 1.1539,
      "step": 1270
    },
    {
      "epoch": 0.38072575847709694,
      "grad_norm": 2.236117124557495,
      "learning_rate": 1.746381122347809e-05,
      "loss": 1.1903,
      "step": 1280
    },
    {
      "epoch": 0.38370017846519927,
      "grad_norm": 2.3265321254730225,
      "learning_rate": 1.744398175689074e-05,
      "loss": 1.1547,
      "step": 1290
    },
    {
      "epoch": 0.3866745984533016,
      "grad_norm": 2.7216410636901855,
      "learning_rate": 1.7424152290303392e-05,
      "loss": 1.1909,
      "step": 1300
    },
    {
      "epoch": 0.3896490184414039,
      "grad_norm": 3.254410982131958,
      "learning_rate": 1.7404322823716043e-05,
      "loss": 1.3845,
      "step": 1310
    },
    {
      "epoch": 0.39262343842950626,
      "grad_norm": 2.833395004272461,
      "learning_rate": 1.7384493357128694e-05,
      "loss": 1.1997,
      "step": 1320
    },
    {
      "epoch": 0.3955978584176086,
      "grad_norm": 3.2698988914489746,
      "learning_rate": 1.7364663890541346e-05,
      "loss": 1.3446,
      "step": 1330
    },
    {
      "epoch": 0.3985722784057109,
      "grad_norm": 2.3403382301330566,
      "learning_rate": 1.7344834423953997e-05,
      "loss": 1.0143,
      "step": 1340
    },
    {
      "epoch": 0.4015466983938132,
      "grad_norm": 3.229649305343628,
      "learning_rate": 1.7325004957366648e-05,
      "loss": 1.3266,
      "step": 1350
    },
    {
      "epoch": 0.4045211183819155,
      "grad_norm": 4.034514427185059,
      "learning_rate": 1.73051754907793e-05,
      "loss": 1.2192,
      "step": 1360
    },
    {
      "epoch": 0.40749553837001784,
      "grad_norm": 2.7706727981567383,
      "learning_rate": 1.728534602419195e-05,
      "loss": 1.2342,
      "step": 1370
    },
    {
      "epoch": 0.41046995835812017,
      "grad_norm": 3.0133345127105713,
      "learning_rate": 1.72655165576046e-05,
      "loss": 1.1409,
      "step": 1380
    },
    {
      "epoch": 0.4134443783462225,
      "grad_norm": 9.679278373718262,
      "learning_rate": 1.7245687091017252e-05,
      "loss": 1.244,
      "step": 1390
    },
    {
      "epoch": 0.4164187983343248,
      "grad_norm": 3.2326178550720215,
      "learning_rate": 1.7225857624429903e-05,
      "loss": 1.3503,
      "step": 1400
    },
    {
      "epoch": 0.41939321832242715,
      "grad_norm": 2.9454379081726074,
      "learning_rate": 1.7206028157842554e-05,
      "loss": 1.2325,
      "step": 1410
    },
    {
      "epoch": 0.4223676383105294,
      "grad_norm": 2.0592710971832275,
      "learning_rate": 1.7186198691255205e-05,
      "loss": 1.2225,
      "step": 1420
    },
    {
      "epoch": 0.42534205829863175,
      "grad_norm": 4.691318511962891,
      "learning_rate": 1.7166369224667857e-05,
      "loss": 1.3139,
      "step": 1430
    },
    {
      "epoch": 0.4283164782867341,
      "grad_norm": 1.992447853088379,
      "learning_rate": 1.714653975808051e-05,
      "loss": 1.235,
      "step": 1440
    },
    {
      "epoch": 0.4312908982748364,
      "grad_norm": 3.6561129093170166,
      "learning_rate": 1.7126710291493162e-05,
      "loss": 1.2428,
      "step": 1450
    },
    {
      "epoch": 0.43426531826293874,
      "grad_norm": 2.705336093902588,
      "learning_rate": 1.7106880824905813e-05,
      "loss": 1.1943,
      "step": 1460
    },
    {
      "epoch": 0.43723973825104107,
      "grad_norm": 2.942883253097534,
      "learning_rate": 1.7087051358318464e-05,
      "loss": 1.1742,
      "step": 1470
    },
    {
      "epoch": 0.4402141582391434,
      "grad_norm": 3.534162998199463,
      "learning_rate": 1.7067221891731115e-05,
      "loss": 1.1961,
      "step": 1480
    },
    {
      "epoch": 0.44318857822724567,
      "grad_norm": 2.556631565093994,
      "learning_rate": 1.7047392425143766e-05,
      "loss": 1.169,
      "step": 1490
    },
    {
      "epoch": 0.446162998215348,
      "grad_norm": 2.82983136177063,
      "learning_rate": 1.7027562958556418e-05,
      "loss": 1.1312,
      "step": 1500
    },
    {
      "epoch": 0.4491374182034503,
      "grad_norm": 4.196559906005859,
      "learning_rate": 1.700773349196907e-05,
      "loss": 1.1144,
      "step": 1510
    },
    {
      "epoch": 0.45211183819155265,
      "grad_norm": 2.4887912273406982,
      "learning_rate": 1.698790402538172e-05,
      "loss": 1.2938,
      "step": 1520
    },
    {
      "epoch": 0.455086258179655,
      "grad_norm": 2.8393170833587646,
      "learning_rate": 1.696807455879437e-05,
      "loss": 1.2894,
      "step": 1530
    },
    {
      "epoch": 0.4580606781677573,
      "grad_norm": 2.8767173290252686,
      "learning_rate": 1.6948245092207022e-05,
      "loss": 1.3519,
      "step": 1540
    },
    {
      "epoch": 0.46103509815585964,
      "grad_norm": 3.7610459327697754,
      "learning_rate": 1.6928415625619673e-05,
      "loss": 1.1404,
      "step": 1550
    },
    {
      "epoch": 0.4640095181439619,
      "grad_norm": 1.5986406803131104,
      "learning_rate": 1.6908586159032324e-05,
      "loss": 1.1247,
      "step": 1560
    },
    {
      "epoch": 0.46698393813206424,
      "grad_norm": 2.295949935913086,
      "learning_rate": 1.6888756692444975e-05,
      "loss": 1.0932,
      "step": 1570
    },
    {
      "epoch": 0.46995835812016656,
      "grad_norm": 2.8671982288360596,
      "learning_rate": 1.6868927225857626e-05,
      "loss": 1.2111,
      "step": 1580
    },
    {
      "epoch": 0.4729327781082689,
      "grad_norm": 3.591658353805542,
      "learning_rate": 1.6849097759270277e-05,
      "loss": 1.1514,
      "step": 1590
    },
    {
      "epoch": 0.4759071980963712,
      "grad_norm": 2.2555441856384277,
      "learning_rate": 1.682926829268293e-05,
      "loss": 1.2151,
      "step": 1600
    },
    {
      "epoch": 0.47888161808447355,
      "grad_norm": 2.073793411254883,
      "learning_rate": 1.680943882609558e-05,
      "loss": 1.0451,
      "step": 1610
    },
    {
      "epoch": 0.4818560380725758,
      "grad_norm": 2.9053194522857666,
      "learning_rate": 1.678960935950823e-05,
      "loss": 1.3188,
      "step": 1620
    },
    {
      "epoch": 0.48483045806067815,
      "grad_norm": 2.6762611865997314,
      "learning_rate": 1.6769779892920882e-05,
      "loss": 1.1284,
      "step": 1630
    },
    {
      "epoch": 0.4878048780487805,
      "grad_norm": 2.5872132778167725,
      "learning_rate": 1.6749950426333533e-05,
      "loss": 1.065,
      "step": 1640
    },
    {
      "epoch": 0.4907792980368828,
      "grad_norm": 4.487478256225586,
      "learning_rate": 1.6730120959746184e-05,
      "loss": 1.318,
      "step": 1650
    },
    {
      "epoch": 0.49375371802498513,
      "grad_norm": 7.294075012207031,
      "learning_rate": 1.6710291493158835e-05,
      "loss": 1.2472,
      "step": 1660
    },
    {
      "epoch": 0.49672813801308746,
      "grad_norm": 3.407980442047119,
      "learning_rate": 1.6690462026571486e-05,
      "loss": 1.2023,
      "step": 1670
    },
    {
      "epoch": 0.4997025580011898,
      "grad_norm": 2.4925153255462646,
      "learning_rate": 1.6670632559984137e-05,
      "loss": 1.1582,
      "step": 1680
    },
    {
      "epoch": 0.5026769779892921,
      "grad_norm": 2.4686458110809326,
      "learning_rate": 1.665080309339679e-05,
      "loss": 1.123,
      "step": 1690
    },
    {
      "epoch": 0.5056513979773944,
      "grad_norm": 2.2136924266815186,
      "learning_rate": 1.6630973626809443e-05,
      "loss": 1.1223,
      "step": 1700
    },
    {
      "epoch": 0.5086258179654968,
      "grad_norm": 2.3016841411590576,
      "learning_rate": 1.6611144160222094e-05,
      "loss": 1.1318,
      "step": 1710
    },
    {
      "epoch": 0.5116002379535991,
      "grad_norm": 6.353873252868652,
      "learning_rate": 1.6591314693634745e-05,
      "loss": 1.1027,
      "step": 1720
    },
    {
      "epoch": 0.5145746579417013,
      "grad_norm": 2.4481773376464844,
      "learning_rate": 1.6571485227047396e-05,
      "loss": 0.9874,
      "step": 1730
    },
    {
      "epoch": 0.5175490779298036,
      "grad_norm": 2.1873154640197754,
      "learning_rate": 1.6551655760460047e-05,
      "loss": 1.0799,
      "step": 1740
    },
    {
      "epoch": 0.520523497917906,
      "grad_norm": 2.3905773162841797,
      "learning_rate": 1.6531826293872698e-05,
      "loss": 1.0523,
      "step": 1750
    },
    {
      "epoch": 0.5234979179060083,
      "grad_norm": 2.0751969814300537,
      "learning_rate": 1.651199682728535e-05,
      "loss": 1.1077,
      "step": 1760
    },
    {
      "epoch": 0.5264723378941106,
      "grad_norm": 2.945545196533203,
      "learning_rate": 1.6492167360698e-05,
      "loss": 1.1648,
      "step": 1770
    },
    {
      "epoch": 0.529446757882213,
      "grad_norm": 2.3653204441070557,
      "learning_rate": 1.647233789411065e-05,
      "loss": 1.2321,
      "step": 1780
    },
    {
      "epoch": 0.5324211778703153,
      "grad_norm": 2.5945520401000977,
      "learning_rate": 1.6452508427523303e-05,
      "loss": 1.217,
      "step": 1790
    },
    {
      "epoch": 0.5353955978584176,
      "grad_norm": 2.4531054496765137,
      "learning_rate": 1.6432678960935954e-05,
      "loss": 1.0605,
      "step": 1800
    },
    {
      "epoch": 0.5383700178465199,
      "grad_norm": 6.198512077331543,
      "learning_rate": 1.6412849494348605e-05,
      "loss": 1.1983,
      "step": 1810
    },
    {
      "epoch": 0.5413444378346223,
      "grad_norm": 1.7238459587097168,
      "learning_rate": 1.6393020027761256e-05,
      "loss": 0.9451,
      "step": 1820
    },
    {
      "epoch": 0.5443188578227246,
      "grad_norm": 3.2935354709625244,
      "learning_rate": 1.6373190561173907e-05,
      "loss": 1.1162,
      "step": 1830
    },
    {
      "epoch": 0.5472932778108269,
      "grad_norm": 3.350724458694458,
      "learning_rate": 1.6353361094586558e-05,
      "loss": 1.1745,
      "step": 1840
    },
    {
      "epoch": 0.5502676977989293,
      "grad_norm": 2.844966173171997,
      "learning_rate": 1.633353162799921e-05,
      "loss": 1.3556,
      "step": 1850
    },
    {
      "epoch": 0.5532421177870316,
      "grad_norm": 2.185089588165283,
      "learning_rate": 1.631370216141186e-05,
      "loss": 1.2276,
      "step": 1860
    },
    {
      "epoch": 0.5562165377751338,
      "grad_norm": 2.5907702445983887,
      "learning_rate": 1.629387269482451e-05,
      "loss": 1.0734,
      "step": 1870
    },
    {
      "epoch": 0.5591909577632361,
      "grad_norm": 2.6680688858032227,
      "learning_rate": 1.6274043228237163e-05,
      "loss": 1.1388,
      "step": 1880
    },
    {
      "epoch": 0.5621653777513385,
      "grad_norm": 3.5789692401885986,
      "learning_rate": 1.6254213761649814e-05,
      "loss": 1.2516,
      "step": 1890
    },
    {
      "epoch": 0.5651397977394408,
      "grad_norm": 2.7315726280212402,
      "learning_rate": 1.6234384295062465e-05,
      "loss": 1.1251,
      "step": 1900
    },
    {
      "epoch": 0.5681142177275431,
      "grad_norm": 2.2416226863861084,
      "learning_rate": 1.6214554828475116e-05,
      "loss": 1.1228,
      "step": 1910
    },
    {
      "epoch": 0.5710886377156454,
      "grad_norm": 2.324265480041504,
      "learning_rate": 1.6194725361887767e-05,
      "loss": 1.1828,
      "step": 1920
    },
    {
      "epoch": 0.5740630577037478,
      "grad_norm": 18.234651565551758,
      "learning_rate": 1.6174895895300418e-05,
      "loss": 1.047,
      "step": 1930
    },
    {
      "epoch": 0.5770374776918501,
      "grad_norm": 1.923673391342163,
      "learning_rate": 1.615506642871307e-05,
      "loss": 1.1765,
      "step": 1940
    },
    {
      "epoch": 0.5800118976799524,
      "grad_norm": 2.5075411796569824,
      "learning_rate": 1.613523696212572e-05,
      "loss": 1.2175,
      "step": 1950
    },
    {
      "epoch": 0.5829863176680548,
      "grad_norm": 2.4489705562591553,
      "learning_rate": 1.611540749553837e-05,
      "loss": 1.0586,
      "step": 1960
    },
    {
      "epoch": 0.5859607376561571,
      "grad_norm": 3.1693198680877686,
      "learning_rate": 1.6095578028951022e-05,
      "loss": 1.3614,
      "step": 1970
    },
    {
      "epoch": 0.5889351576442594,
      "grad_norm": 1.9570279121398926,
      "learning_rate": 1.6075748562363673e-05,
      "loss": 1.1754,
      "step": 1980
    },
    {
      "epoch": 0.5919095776323617,
      "grad_norm": 2.905691146850586,
      "learning_rate": 1.6055919095776325e-05,
      "loss": 1.1288,
      "step": 1990
    },
    {
      "epoch": 0.594883997620464,
      "grad_norm": 2.982105255126953,
      "learning_rate": 1.6036089629188976e-05,
      "loss": 1.0788,
      "step": 2000
    },
    {
      "epoch": 0.5978584176085663,
      "grad_norm": 1.9071041345596313,
      "learning_rate": 1.6016260162601627e-05,
      "loss": 1.0396,
      "step": 2010
    },
    {
      "epoch": 0.6008328375966686,
      "grad_norm": 2.517090082168579,
      "learning_rate": 1.5996430696014278e-05,
      "loss": 0.9884,
      "step": 2020
    },
    {
      "epoch": 0.6038072575847709,
      "grad_norm": 2.1590311527252197,
      "learning_rate": 1.597660122942693e-05,
      "loss": 1.1077,
      "step": 2030
    },
    {
      "epoch": 0.6067816775728733,
      "grad_norm": 3.112117290496826,
      "learning_rate": 1.595677176283958e-05,
      "loss": 1.1244,
      "step": 2040
    },
    {
      "epoch": 0.6097560975609756,
      "grad_norm": 2.2660200595855713,
      "learning_rate": 1.593694229625223e-05,
      "loss": 1.0491,
      "step": 2050
    },
    {
      "epoch": 0.6127305175490779,
      "grad_norm": 2.884387254714966,
      "learning_rate": 1.5917112829664882e-05,
      "loss": 1.0347,
      "step": 2060
    },
    {
      "epoch": 0.6157049375371803,
      "grad_norm": 2.4689760208129883,
      "learning_rate": 1.5897283363077533e-05,
      "loss": 1.0281,
      "step": 2070
    },
    {
      "epoch": 0.6186793575252826,
      "grad_norm": 2.648408889770508,
      "learning_rate": 1.5877453896490184e-05,
      "loss": 1.2797,
      "step": 2080
    },
    {
      "epoch": 0.6216537775133849,
      "grad_norm": 2.3643991947174072,
      "learning_rate": 1.5857624429902835e-05,
      "loss": 1.0929,
      "step": 2090
    },
    {
      "epoch": 0.6246281975014872,
      "grad_norm": 2.5449182987213135,
      "learning_rate": 1.5837794963315487e-05,
      "loss": 1.2512,
      "step": 2100
    },
    {
      "epoch": 0.6276026174895896,
      "grad_norm": 4.735818862915039,
      "learning_rate": 1.581796549672814e-05,
      "loss": 1.1894,
      "step": 2110
    },
    {
      "epoch": 0.6305770374776919,
      "grad_norm": 2.037890672683716,
      "learning_rate": 1.5798136030140792e-05,
      "loss": 1.1193,
      "step": 2120
    },
    {
      "epoch": 0.6335514574657942,
      "grad_norm": 2.9270026683807373,
      "learning_rate": 1.5778306563553443e-05,
      "loss": 1.1617,
      "step": 2130
    },
    {
      "epoch": 0.6365258774538964,
      "grad_norm": 2.311732053756714,
      "learning_rate": 1.5758477096966094e-05,
      "loss": 1.1847,
      "step": 2140
    },
    {
      "epoch": 0.6395002974419988,
      "grad_norm": 2.490755319595337,
      "learning_rate": 1.5738647630378745e-05,
      "loss": 1.1932,
      "step": 2150
    },
    {
      "epoch": 0.6424747174301011,
      "grad_norm": 2.536043643951416,
      "learning_rate": 1.5718818163791397e-05,
      "loss": 1.1215,
      "step": 2160
    },
    {
      "epoch": 0.6454491374182034,
      "grad_norm": 3.2090108394622803,
      "learning_rate": 1.5698988697204048e-05,
      "loss": 1.1992,
      "step": 2170
    },
    {
      "epoch": 0.6484235574063058,
      "grad_norm": 3.2542331218719482,
      "learning_rate": 1.56791592306167e-05,
      "loss": 1.1733,
      "step": 2180
    },
    {
      "epoch": 0.6513979773944081,
      "grad_norm": 2.5031962394714355,
      "learning_rate": 1.565932976402935e-05,
      "loss": 1.0524,
      "step": 2190
    },
    {
      "epoch": 0.6543723973825104,
      "grad_norm": 2.9142589569091797,
      "learning_rate": 1.5639500297442e-05,
      "loss": 1.1823,
      "step": 2200
    },
    {
      "epoch": 0.6573468173706127,
      "grad_norm": 2.0712954998016357,
      "learning_rate": 1.5619670830854652e-05,
      "loss": 1.172,
      "step": 2210
    },
    {
      "epoch": 0.6603212373587151,
      "grad_norm": 2.1472842693328857,
      "learning_rate": 1.5599841364267303e-05,
      "loss": 0.908,
      "step": 2220
    },
    {
      "epoch": 0.6632956573468174,
      "grad_norm": 2.7466132640838623,
      "learning_rate": 1.5580011897679954e-05,
      "loss": 1.1544,
      "step": 2230
    },
    {
      "epoch": 0.6662700773349197,
      "grad_norm": 24.806875228881836,
      "learning_rate": 1.5560182431092605e-05,
      "loss": 1.1445,
      "step": 2240
    },
    {
      "epoch": 0.669244497323022,
      "grad_norm": 2.348313093185425,
      "learning_rate": 1.5540352964505256e-05,
      "loss": 1.0337,
      "step": 2250
    },
    {
      "epoch": 0.6722189173111244,
      "grad_norm": 2.950786828994751,
      "learning_rate": 1.5520523497917907e-05,
      "loss": 1.2605,
      "step": 2260
    },
    {
      "epoch": 0.6751933372992267,
      "grad_norm": 2.47383189201355,
      "learning_rate": 1.550069403133056e-05,
      "loss": 1.1985,
      "step": 2270
    },
    {
      "epoch": 0.6781677572873289,
      "grad_norm": 2.3583619594573975,
      "learning_rate": 1.548086456474321e-05,
      "loss": 1.0958,
      "step": 2280
    },
    {
      "epoch": 0.6811421772754312,
      "grad_norm": 2.495673418045044,
      "learning_rate": 1.546103509815586e-05,
      "loss": 1.0303,
      "step": 2290
    },
    {
      "epoch": 0.6841165972635336,
      "grad_norm": 2.309312343597412,
      "learning_rate": 1.5441205631568512e-05,
      "loss": 1.0135,
      "step": 2300
    },
    {
      "epoch": 0.6870910172516359,
      "grad_norm": 2.3469152450561523,
      "learning_rate": 1.5421376164981163e-05,
      "loss": 1.1717,
      "step": 2310
    },
    {
      "epoch": 0.6900654372397382,
      "grad_norm": 5.972252368927002,
      "learning_rate": 1.5401546698393814e-05,
      "loss": 1.0533,
      "step": 2320
    },
    {
      "epoch": 0.6930398572278406,
      "grad_norm": 2.3936264514923096,
      "learning_rate": 1.5381717231806465e-05,
      "loss": 1.2739,
      "step": 2330
    },
    {
      "epoch": 0.6960142772159429,
      "grad_norm": 1.7732205390930176,
      "learning_rate": 1.5361887765219116e-05,
      "loss": 1.0552,
      "step": 2340
    },
    {
      "epoch": 0.6989886972040452,
      "grad_norm": 4.164812088012695,
      "learning_rate": 1.5342058298631767e-05,
      "loss": 1.1717,
      "step": 2350
    },
    {
      "epoch": 0.7019631171921475,
      "grad_norm": 1.852568507194519,
      "learning_rate": 1.532222883204442e-05,
      "loss": 0.9403,
      "step": 2360
    },
    {
      "epoch": 0.7049375371802499,
      "grad_norm": 4.160287380218506,
      "learning_rate": 1.530239936545707e-05,
      "loss": 0.9097,
      "step": 2370
    },
    {
      "epoch": 0.7079119571683522,
      "grad_norm": 2.4490737915039062,
      "learning_rate": 1.528256989886972e-05,
      "loss": 1.0647,
      "step": 2380
    },
    {
      "epoch": 0.7108863771564545,
      "grad_norm": 5.302168369293213,
      "learning_rate": 1.526274043228237e-05,
      "loss": 1.1861,
      "step": 2390
    },
    {
      "epoch": 0.7138607971445569,
      "grad_norm": 2.374682664871216,
      "learning_rate": 1.5242910965695023e-05,
      "loss": 1.1004,
      "step": 2400
    },
    {
      "epoch": 0.7168352171326592,
      "grad_norm": 2.417161703109741,
      "learning_rate": 1.5223081499107676e-05,
      "loss": 1.0908,
      "step": 2410
    },
    {
      "epoch": 0.7198096371207614,
      "grad_norm": 2.1739513874053955,
      "learning_rate": 1.5203252032520327e-05,
      "loss": 1.1222,
      "step": 2420
    },
    {
      "epoch": 0.7227840571088637,
      "grad_norm": 2.153794288635254,
      "learning_rate": 1.5183422565932978e-05,
      "loss": 1.2044,
      "step": 2430
    },
    {
      "epoch": 0.7257584770969661,
      "grad_norm": 1.9900211095809937,
      "learning_rate": 1.5163593099345629e-05,
      "loss": 1.0642,
      "step": 2440
    },
    {
      "epoch": 0.7287328970850684,
      "grad_norm": 2.2437267303466797,
      "learning_rate": 1.514376363275828e-05,
      "loss": 1.2761,
      "step": 2450
    },
    {
      "epoch": 0.7317073170731707,
      "grad_norm": 3.095172166824341,
      "learning_rate": 1.5123934166170931e-05,
      "loss": 1.2701,
      "step": 2460
    },
    {
      "epoch": 0.734681737061273,
      "grad_norm": 5.763138771057129,
      "learning_rate": 1.5104104699583582e-05,
      "loss": 1.0699,
      "step": 2470
    },
    {
      "epoch": 0.7376561570493754,
      "grad_norm": 2.243908643722534,
      "learning_rate": 1.5084275232996233e-05,
      "loss": 0.884,
      "step": 2480
    },
    {
      "epoch": 0.7406305770374777,
      "grad_norm": 2.426154375076294,
      "learning_rate": 1.5064445766408884e-05,
      "loss": 1.1627,
      "step": 2490
    },
    {
      "epoch": 0.74360499702558,
      "grad_norm": 2.1421918869018555,
      "learning_rate": 1.5044616299821535e-05,
      "loss": 1.1114,
      "step": 2500
    },
    {
      "epoch": 0.7465794170136824,
      "grad_norm": 2.182199239730835,
      "learning_rate": 1.5024786833234187e-05,
      "loss": 1.0733,
      "step": 2510
    },
    {
      "epoch": 0.7495538370017847,
      "grad_norm": 4.896794319152832,
      "learning_rate": 1.5004957366646838e-05,
      "loss": 1.0855,
      "step": 2520
    },
    {
      "epoch": 0.752528256989887,
      "grad_norm": 1.9705830812454224,
      "learning_rate": 1.498512790005949e-05,
      "loss": 1.122,
      "step": 2530
    },
    {
      "epoch": 0.7555026769779893,
      "grad_norm": 2.243126630783081,
      "learning_rate": 1.4965298433472141e-05,
      "loss": 1.1537,
      "step": 2540
    },
    {
      "epoch": 0.7584770969660916,
      "grad_norm": 2.904205799102783,
      "learning_rate": 1.4945468966884793e-05,
      "loss": 1.0979,
      "step": 2550
    },
    {
      "epoch": 0.7614515169541939,
      "grad_norm": 2.4261884689331055,
      "learning_rate": 1.4925639500297444e-05,
      "loss": 0.977,
      "step": 2560
    },
    {
      "epoch": 0.7644259369422962,
      "grad_norm": 2.5026443004608154,
      "learning_rate": 1.4905810033710095e-05,
      "loss": 0.9903,
      "step": 2570
    },
    {
      "epoch": 0.7674003569303985,
      "grad_norm": 2.5773515701293945,
      "learning_rate": 1.4885980567122746e-05,
      "loss": 1.097,
      "step": 2580
    },
    {
      "epoch": 0.7703747769185009,
      "grad_norm": 2.6074116230010986,
      "learning_rate": 1.4866151100535397e-05,
      "loss": 1.0881,
      "step": 2590
    },
    {
      "epoch": 0.7733491969066032,
      "grad_norm": 2.4162917137145996,
      "learning_rate": 1.4846321633948048e-05,
      "loss": 1.122,
      "step": 2600
    },
    {
      "epoch": 0.7763236168947055,
      "grad_norm": 2.1458699703216553,
      "learning_rate": 1.4826492167360699e-05,
      "loss": 1.1047,
      "step": 2610
    },
    {
      "epoch": 0.7792980368828079,
      "grad_norm": 2.825005292892456,
      "learning_rate": 1.480666270077335e-05,
      "loss": 1.2745,
      "step": 2620
    },
    {
      "epoch": 0.7822724568709102,
      "grad_norm": 1.8053439855575562,
      "learning_rate": 1.4786833234186001e-05,
      "loss": 1.0114,
      "step": 2630
    },
    {
      "epoch": 0.7852468768590125,
      "grad_norm": 2.5441455841064453,
      "learning_rate": 1.4767003767598654e-05,
      "loss": 1.0694,
      "step": 2640
    },
    {
      "epoch": 0.7882212968471148,
      "grad_norm": 3.1423330307006836,
      "learning_rate": 1.4747174301011305e-05,
      "loss": 1.1756,
      "step": 2650
    },
    {
      "epoch": 0.7911957168352172,
      "grad_norm": 2.275871515274048,
      "learning_rate": 1.4727344834423956e-05,
      "loss": 1.048,
      "step": 2660
    },
    {
      "epoch": 0.7941701368233195,
      "grad_norm": 2.3337578773498535,
      "learning_rate": 1.4707515367836607e-05,
      "loss": 0.9373,
      "step": 2670
    },
    {
      "epoch": 0.7971445568114218,
      "grad_norm": 2.7815330028533936,
      "learning_rate": 1.4687685901249258e-05,
      "loss": 0.9188,
      "step": 2680
    },
    {
      "epoch": 0.800118976799524,
      "grad_norm": 2.005396842956543,
      "learning_rate": 1.466785643466191e-05,
      "loss": 1.107,
      "step": 2690
    },
    {
      "epoch": 0.8030933967876264,
      "grad_norm": 2.408144235610962,
      "learning_rate": 1.464802696807456e-05,
      "loss": 1.0285,
      "step": 2700
    },
    {
      "epoch": 0.8060678167757287,
      "grad_norm": 2.5621185302734375,
      "learning_rate": 1.4628197501487212e-05,
      "loss": 1.0595,
      "step": 2710
    },
    {
      "epoch": 0.809042236763831,
      "grad_norm": 1.849439263343811,
      "learning_rate": 1.4608368034899863e-05,
      "loss": 0.9808,
      "step": 2720
    },
    {
      "epoch": 0.8120166567519334,
      "grad_norm": 2.266838788986206,
      "learning_rate": 1.4588538568312514e-05,
      "loss": 1.0892,
      "step": 2730
    },
    {
      "epoch": 0.8149910767400357,
      "grad_norm": 9.509018898010254,
      "learning_rate": 1.4568709101725165e-05,
      "loss": 1.1422,
      "step": 2740
    },
    {
      "epoch": 0.817965496728138,
      "grad_norm": 2.856926441192627,
      "learning_rate": 1.4548879635137816e-05,
      "loss": 1.0419,
      "step": 2750
    },
    {
      "epoch": 0.8209399167162403,
      "grad_norm": 2.4882001876831055,
      "learning_rate": 1.4529050168550467e-05,
      "loss": 1.137,
      "step": 2760
    },
    {
      "epoch": 0.8239143367043427,
      "grad_norm": 2.5866878032684326,
      "learning_rate": 1.4509220701963118e-05,
      "loss": 1.0367,
      "step": 2770
    },
    {
      "epoch": 0.826888756692445,
      "grad_norm": 2.7857933044433594,
      "learning_rate": 1.448939123537577e-05,
      "loss": 1.0708,
      "step": 2780
    },
    {
      "epoch": 0.8298631766805473,
      "grad_norm": 2.3976426124572754,
      "learning_rate": 1.446956176878842e-05,
      "loss": 0.9821,
      "step": 2790
    },
    {
      "epoch": 0.8328375966686496,
      "grad_norm": 2.3541207313537598,
      "learning_rate": 1.4449732302201072e-05,
      "loss": 0.9002,
      "step": 2800
    },
    {
      "epoch": 0.835812016656752,
      "grad_norm": 3.6978249549865723,
      "learning_rate": 1.4429902835613723e-05,
      "loss": 1.0156,
      "step": 2810
    },
    {
      "epoch": 0.8387864366448543,
      "grad_norm": 2.122593641281128,
      "learning_rate": 1.4410073369026374e-05,
      "loss": 1.0146,
      "step": 2820
    },
    {
      "epoch": 0.8417608566329565,
      "grad_norm": 2.434551954269409,
      "learning_rate": 1.4390243902439025e-05,
      "loss": 1.0837,
      "step": 2830
    },
    {
      "epoch": 0.8447352766210589,
      "grad_norm": 13.30216121673584,
      "learning_rate": 1.4370414435851676e-05,
      "loss": 1.0649,
      "step": 2840
    },
    {
      "epoch": 0.8477096966091612,
      "grad_norm": 1.7501590251922607,
      "learning_rate": 1.4350584969264327e-05,
      "loss": 1.1625,
      "step": 2850
    },
    {
      "epoch": 0.8506841165972635,
      "grad_norm": 2.080801486968994,
      "learning_rate": 1.4330755502676978e-05,
      "loss": 1.2266,
      "step": 2860
    },
    {
      "epoch": 0.8536585365853658,
      "grad_norm": 5.671804428100586,
      "learning_rate": 1.431092603608963e-05,
      "loss": 0.9806,
      "step": 2870
    },
    {
      "epoch": 0.8566329565734682,
      "grad_norm": 2.7558906078338623,
      "learning_rate": 1.429109656950228e-05,
      "loss": 1.0958,
      "step": 2880
    },
    {
      "epoch": 0.8596073765615705,
      "grad_norm": 2.2716963291168213,
      "learning_rate": 1.4271267102914931e-05,
      "loss": 1.0777,
      "step": 2890
    },
    {
      "epoch": 0.8625817965496728,
      "grad_norm": 2.960343599319458,
      "learning_rate": 1.4251437636327583e-05,
      "loss": 0.9825,
      "step": 2900
    },
    {
      "epoch": 0.8655562165377751,
      "grad_norm": 1.745735764503479,
      "learning_rate": 1.4231608169740234e-05,
      "loss": 0.8888,
      "step": 2910
    },
    {
      "epoch": 0.8685306365258775,
      "grad_norm": 2.3653459548950195,
      "learning_rate": 1.4211778703152885e-05,
      "loss": 1.1079,
      "step": 2920
    },
    {
      "epoch": 0.8715050565139798,
      "grad_norm": 2.918389320373535,
      "learning_rate": 1.4191949236565536e-05,
      "loss": 1.0822,
      "step": 2930
    },
    {
      "epoch": 0.8744794765020821,
      "grad_norm": 2.3172712326049805,
      "learning_rate": 1.417211976997819e-05,
      "loss": 1.1991,
      "step": 2940
    },
    {
      "epoch": 0.8774538964901845,
      "grad_norm": 2.796628952026367,
      "learning_rate": 1.4152290303390841e-05,
      "loss": 1.0541,
      "step": 2950
    },
    {
      "epoch": 0.8804283164782868,
      "grad_norm": 3.28886079788208,
      "learning_rate": 1.4132460836803493e-05,
      "loss": 1.062,
      "step": 2960
    },
    {
      "epoch": 0.883402736466389,
      "grad_norm": 1.8694053888320923,
      "learning_rate": 1.4112631370216144e-05,
      "loss": 1.0342,
      "step": 2970
    },
    {
      "epoch": 0.8863771564544913,
      "grad_norm": 1.8932523727416992,
      "learning_rate": 1.4092801903628795e-05,
      "loss": 1.1141,
      "step": 2980
    },
    {
      "epoch": 0.8893515764425937,
      "grad_norm": 1.9527273178100586,
      "learning_rate": 1.4072972437041446e-05,
      "loss": 1.0235,
      "step": 2990
    },
    {
      "epoch": 0.892325996430696,
      "grad_norm": 2.3412673473358154,
      "learning_rate": 1.4053142970454097e-05,
      "loss": 1.1055,
      "step": 3000
    },
    {
      "epoch": 0.8953004164187983,
      "grad_norm": 2.4447178840637207,
      "learning_rate": 1.4033313503866748e-05,
      "loss": 1.1957,
      "step": 3010
    },
    {
      "epoch": 0.8982748364069006,
      "grad_norm": 3.0534911155700684,
      "learning_rate": 1.4013484037279399e-05,
      "loss": 1.1794,
      "step": 3020
    },
    {
      "epoch": 0.901249256395003,
      "grad_norm": 2.611084222793579,
      "learning_rate": 1.399365457069205e-05,
      "loss": 0.9684,
      "step": 3030
    },
    {
      "epoch": 0.9042236763831053,
      "grad_norm": 2.0673205852508545,
      "learning_rate": 1.3973825104104701e-05,
      "loss": 1.0496,
      "step": 3040
    },
    {
      "epoch": 0.9071980963712076,
      "grad_norm": 2.013244390487671,
      "learning_rate": 1.3953995637517352e-05,
      "loss": 0.9221,
      "step": 3050
    },
    {
      "epoch": 0.91017251635931,
      "grad_norm": 2.621508836746216,
      "learning_rate": 1.3934166170930003e-05,
      "loss": 0.9793,
      "step": 3060
    },
    {
      "epoch": 0.9131469363474123,
      "grad_norm": 2.225829601287842,
      "learning_rate": 1.3914336704342655e-05,
      "loss": 1.0095,
      "step": 3070
    },
    {
      "epoch": 0.9161213563355146,
      "grad_norm": 2.9685020446777344,
      "learning_rate": 1.3894507237755306e-05,
      "loss": 1.1101,
      "step": 3080
    },
    {
      "epoch": 0.9190957763236169,
      "grad_norm": 2.9847049713134766,
      "learning_rate": 1.3874677771167957e-05,
      "loss": 0.9937,
      "step": 3090
    },
    {
      "epoch": 0.9220701963117193,
      "grad_norm": 2.1173758506774902,
      "learning_rate": 1.3854848304580608e-05,
      "loss": 1.0475,
      "step": 3100
    },
    {
      "epoch": 0.9250446162998215,
      "grad_norm": 2.389523506164551,
      "learning_rate": 1.3835018837993259e-05,
      "loss": 1.1657,
      "step": 3110
    },
    {
      "epoch": 0.9280190362879238,
      "grad_norm": 2.3684425354003906,
      "learning_rate": 1.381518937140591e-05,
      "loss": 1.0225,
      "step": 3120
    },
    {
      "epoch": 0.9309934562760261,
      "grad_norm": 2.157345771789551,
      "learning_rate": 1.3795359904818561e-05,
      "loss": 1.0291,
      "step": 3130
    },
    {
      "epoch": 0.9339678762641285,
      "grad_norm": 2.4556326866149902,
      "learning_rate": 1.3775530438231212e-05,
      "loss": 1.0321,
      "step": 3140
    },
    {
      "epoch": 0.9369422962522308,
      "grad_norm": 3.7621381282806396,
      "learning_rate": 1.3755700971643863e-05,
      "loss": 1.0173,
      "step": 3150
    },
    {
      "epoch": 0.9399167162403331,
      "grad_norm": 2.3169569969177246,
      "learning_rate": 1.3735871505056514e-05,
      "loss": 1.0253,
      "step": 3160
    },
    {
      "epoch": 0.9428911362284355,
      "grad_norm": 2.6958515644073486,
      "learning_rate": 1.3716042038469165e-05,
      "loss": 1.133,
      "step": 3170
    },
    {
      "epoch": 0.9458655562165378,
      "grad_norm": 4.848586082458496,
      "learning_rate": 1.3696212571881817e-05,
      "loss": 1.0458,
      "step": 3180
    },
    {
      "epoch": 0.9488399762046401,
      "grad_norm": 2.2110934257507324,
      "learning_rate": 1.3676383105294468e-05,
      "loss": 0.9827,
      "step": 3190
    },
    {
      "epoch": 0.9518143961927424,
      "grad_norm": 2.8929805755615234,
      "learning_rate": 1.3656553638707119e-05,
      "loss": 1.0446,
      "step": 3200
    },
    {
      "epoch": 0.9547888161808448,
      "grad_norm": 2.0052590370178223,
      "learning_rate": 1.3636724172119772e-05,
      "loss": 1.0129,
      "step": 3210
    },
    {
      "epoch": 0.9577632361689471,
      "grad_norm": 1.9813134670257568,
      "learning_rate": 1.3616894705532423e-05,
      "loss": 0.9522,
      "step": 3220
    },
    {
      "epoch": 0.9607376561570494,
      "grad_norm": 1.9601584672927856,
      "learning_rate": 1.3597065238945074e-05,
      "loss": 0.8701,
      "step": 3230
    },
    {
      "epoch": 0.9637120761451516,
      "grad_norm": 2.295536994934082,
      "learning_rate": 1.3577235772357725e-05,
      "loss": 0.9754,
      "step": 3240
    },
    {
      "epoch": 0.966686496133254,
      "grad_norm": 2.5937657356262207,
      "learning_rate": 1.3557406305770376e-05,
      "loss": 1.081,
      "step": 3250
    },
    {
      "epoch": 0.9696609161213563,
      "grad_norm": 2.664574384689331,
      "learning_rate": 1.3537576839183027e-05,
      "loss": 1.0347,
      "step": 3260
    },
    {
      "epoch": 0.9726353361094586,
      "grad_norm": 2.3376541137695312,
      "learning_rate": 1.3517747372595678e-05,
      "loss": 1.0614,
      "step": 3270
    },
    {
      "epoch": 0.975609756097561,
      "grad_norm": 1.8858277797698975,
      "learning_rate": 1.349791790600833e-05,
      "loss": 0.9613,
      "step": 3280
    },
    {
      "epoch": 0.9785841760856633,
      "grad_norm": 4.011244297027588,
      "learning_rate": 1.347808843942098e-05,
      "loss": 1.1117,
      "step": 3290
    },
    {
      "epoch": 0.9815585960737656,
      "grad_norm": 2.174544095993042,
      "learning_rate": 1.3458258972833631e-05,
      "loss": 1.0282,
      "step": 3300
    },
    {
      "epoch": 0.9845330160618679,
      "grad_norm": 2.186791181564331,
      "learning_rate": 1.3438429506246282e-05,
      "loss": 0.9913,
      "step": 3310
    },
    {
      "epoch": 0.9875074360499703,
      "grad_norm": 2.5972936153411865,
      "learning_rate": 1.3418600039658934e-05,
      "loss": 1.0387,
      "step": 3320
    },
    {
      "epoch": 0.9904818560380726,
      "grad_norm": 12.044614791870117,
      "learning_rate": 1.3398770573071585e-05,
      "loss": 1.0881,
      "step": 3330
    },
    {
      "epoch": 0.9934562760261749,
      "grad_norm": 2.6336209774017334,
      "learning_rate": 1.3378941106484236e-05,
      "loss": 0.9865,
      "step": 3340
    },
    {
      "epoch": 0.9964306960142773,
      "grad_norm": 2.4936459064483643,
      "learning_rate": 1.3359111639896889e-05,
      "loss": 1.0581,
      "step": 3350
    },
    {
      "epoch": 0.9994051160023796,
      "grad_norm": 1.956563949584961,
      "learning_rate": 1.333928217330954e-05,
      "loss": 0.9734,
      "step": 3360
    },
    {
      "epoch": 1.0,
      "eval_bleu": 0.2747968571736394,
      "eval_loss": 0.905328094959259,
      "eval_runtime": 152.6856,
      "eval_samples_per_second": 9.785,
      "eval_steps_per_second": 2.449,
      "step": 3362
    },
    {
      "epoch": 1.002379535990482,
      "grad_norm": 1.8478902578353882,
      "learning_rate": 1.331945270672219e-05,
      "loss": 0.9648,
      "step": 3370
    },
    {
      "epoch": 1.0053539559785842,
      "grad_norm": 1.9964100122451782,
      "learning_rate": 1.3299623240134842e-05,
      "loss": 1.0622,
      "step": 3380
    },
    {
      "epoch": 1.0083283759666866,
      "grad_norm": 2.3625638484954834,
      "learning_rate": 1.3279793773547493e-05,
      "loss": 0.8964,
      "step": 3390
    },
    {
      "epoch": 1.011302795954789,
      "grad_norm": 2.133871555328369,
      "learning_rate": 1.3259964306960144e-05,
      "loss": 1.0497,
      "step": 3400
    },
    {
      "epoch": 1.0142772159428912,
      "grad_norm": 1.8475555181503296,
      "learning_rate": 1.3240134840372795e-05,
      "loss": 1.146,
      "step": 3410
    },
    {
      "epoch": 1.0172516359309935,
      "grad_norm": 2.48197340965271,
      "learning_rate": 1.3220305373785446e-05,
      "loss": 0.9338,
      "step": 3420
    },
    {
      "epoch": 1.0202260559190959,
      "grad_norm": 2.8050968647003174,
      "learning_rate": 1.3200475907198097e-05,
      "loss": 1.0836,
      "step": 3430
    },
    {
      "epoch": 1.0232004759071982,
      "grad_norm": 2.135615110397339,
      "learning_rate": 1.318064644061075e-05,
      "loss": 0.9046,
      "step": 3440
    },
    {
      "epoch": 1.0261748958953003,
      "grad_norm": 2.244197368621826,
      "learning_rate": 1.3160816974023401e-05,
      "loss": 1.0643,
      "step": 3450
    },
    {
      "epoch": 1.0291493158834026,
      "grad_norm": 2.0484726428985596,
      "learning_rate": 1.3140987507436052e-05,
      "loss": 0.8781,
      "step": 3460
    },
    {
      "epoch": 1.032123735871505,
      "grad_norm": 2.224973440170288,
      "learning_rate": 1.3121158040848703e-05,
      "loss": 1.022,
      "step": 3470
    },
    {
      "epoch": 1.0350981558596073,
      "grad_norm": 2.4019110202789307,
      "learning_rate": 1.3101328574261354e-05,
      "loss": 0.9865,
      "step": 3480
    },
    {
      "epoch": 1.0380725758477096,
      "grad_norm": 1.802816390991211,
      "learning_rate": 1.3081499107674006e-05,
      "loss": 0.9177,
      "step": 3490
    },
    {
      "epoch": 1.041046995835812,
      "grad_norm": 2.171182155609131,
      "learning_rate": 1.3061669641086657e-05,
      "loss": 1.0803,
      "step": 3500
    },
    {
      "epoch": 1.0440214158239143,
      "grad_norm": 2.1811161041259766,
      "learning_rate": 1.3041840174499308e-05,
      "loss": 1.1289,
      "step": 3510
    },
    {
      "epoch": 1.0469958358120166,
      "grad_norm": 2.3826119899749756,
      "learning_rate": 1.3022010707911959e-05,
      "loss": 1.0334,
      "step": 3520
    },
    {
      "epoch": 1.049970255800119,
      "grad_norm": 3.41223406791687,
      "learning_rate": 1.300218124132461e-05,
      "loss": 1.0384,
      "step": 3530
    },
    {
      "epoch": 1.0529446757882213,
      "grad_norm": 2.81241512298584,
      "learning_rate": 1.2982351774737261e-05,
      "loss": 1.1289,
      "step": 3540
    },
    {
      "epoch": 1.0559190957763236,
      "grad_norm": 2.216033458709717,
      "learning_rate": 1.2962522308149912e-05,
      "loss": 0.9356,
      "step": 3550
    },
    {
      "epoch": 1.058893515764426,
      "grad_norm": 2.0935657024383545,
      "learning_rate": 1.2942692841562563e-05,
      "loss": 1.0731,
      "step": 3560
    },
    {
      "epoch": 1.0618679357525282,
      "grad_norm": 5.541337013244629,
      "learning_rate": 1.2922863374975214e-05,
      "loss": 0.8609,
      "step": 3570
    },
    {
      "epoch": 1.0648423557406306,
      "grad_norm": 2.868932008743286,
      "learning_rate": 1.2903033908387865e-05,
      "loss": 1.0217,
      "step": 3580
    },
    {
      "epoch": 1.067816775728733,
      "grad_norm": 2.5545055866241455,
      "learning_rate": 1.2883204441800517e-05,
      "loss": 1.0862,
      "step": 3590
    },
    {
      "epoch": 1.0707911957168352,
      "grad_norm": 2.066006660461426,
      "learning_rate": 1.2863374975213168e-05,
      "loss": 0.8782,
      "step": 3600
    },
    {
      "epoch": 1.0737656157049376,
      "grad_norm": 2.506295680999756,
      "learning_rate": 1.2843545508625819e-05,
      "loss": 1.1274,
      "step": 3610
    },
    {
      "epoch": 1.0767400356930399,
      "grad_norm": 1.666845440864563,
      "learning_rate": 1.282371604203847e-05,
      "loss": 1.0172,
      "step": 3620
    },
    {
      "epoch": 1.0797144556811422,
      "grad_norm": 2.746426582336426,
      "learning_rate": 1.2803886575451121e-05,
      "loss": 1.0456,
      "step": 3630
    },
    {
      "epoch": 1.0826888756692445,
      "grad_norm": 1.8426942825317383,
      "learning_rate": 1.2784057108863772e-05,
      "loss": 1.032,
      "step": 3640
    },
    {
      "epoch": 1.0856632956573469,
      "grad_norm": 2.7142341136932373,
      "learning_rate": 1.2764227642276423e-05,
      "loss": 0.9948,
      "step": 3650
    },
    {
      "epoch": 1.0886377156454492,
      "grad_norm": 2.126866102218628,
      "learning_rate": 1.2744398175689074e-05,
      "loss": 0.9411,
      "step": 3660
    },
    {
      "epoch": 1.0916121356335515,
      "grad_norm": 2.4592809677124023,
      "learning_rate": 1.2724568709101725e-05,
      "loss": 0.9882,
      "step": 3670
    },
    {
      "epoch": 1.0945865556216539,
      "grad_norm": 1.7657636404037476,
      "learning_rate": 1.2704739242514376e-05,
      "loss": 1.0011,
      "step": 3680
    },
    {
      "epoch": 1.0975609756097562,
      "grad_norm": 1.766711711883545,
      "learning_rate": 1.2684909775927027e-05,
      "loss": 0.8487,
      "step": 3690
    },
    {
      "epoch": 1.1005353955978585,
      "grad_norm": 4.211288928985596,
      "learning_rate": 1.2665080309339679e-05,
      "loss": 1.007,
      "step": 3700
    },
    {
      "epoch": 1.1035098155859608,
      "grad_norm": 2.3634817600250244,
      "learning_rate": 1.264525084275233e-05,
      "loss": 1.0099,
      "step": 3710
    },
    {
      "epoch": 1.106484235574063,
      "grad_norm": 2.2825794219970703,
      "learning_rate": 1.262542137616498e-05,
      "loss": 0.9479,
      "step": 3720
    },
    {
      "epoch": 1.1094586555621653,
      "grad_norm": 2.29438853263855,
      "learning_rate": 1.2605591909577632e-05,
      "loss": 1.0709,
      "step": 3730
    },
    {
      "epoch": 1.1124330755502676,
      "grad_norm": 2.814239740371704,
      "learning_rate": 1.2585762442990283e-05,
      "loss": 0.8889,
      "step": 3740
    },
    {
      "epoch": 1.11540749553837,
      "grad_norm": 1.9499220848083496,
      "learning_rate": 1.2565932976402934e-05,
      "loss": 0.8745,
      "step": 3750
    },
    {
      "epoch": 1.1183819155264723,
      "grad_norm": 1.6009458303451538,
      "learning_rate": 1.2546103509815585e-05,
      "loss": 0.9052,
      "step": 3760
    },
    {
      "epoch": 1.1213563355145746,
      "grad_norm": 2.0834107398986816,
      "learning_rate": 1.252627404322824e-05,
      "loss": 0.9167,
      "step": 3770
    },
    {
      "epoch": 1.124330755502677,
      "grad_norm": 2.285870313644409,
      "learning_rate": 1.250644457664089e-05,
      "loss": 0.973,
      "step": 3780
    },
    {
      "epoch": 1.1273051754907792,
      "grad_norm": 2.204322338104248,
      "learning_rate": 1.2486615110053542e-05,
      "loss": 1.0668,
      "step": 3790
    },
    {
      "epoch": 1.1302795954788816,
      "grad_norm": 2.2486071586608887,
      "learning_rate": 1.2466785643466193e-05,
      "loss": 1.0166,
      "step": 3800
    },
    {
      "epoch": 1.133254015466984,
      "grad_norm": 1.9864786863327026,
      "learning_rate": 1.2446956176878844e-05,
      "loss": 0.9421,
      "step": 3810
    },
    {
      "epoch": 1.1362284354550862,
      "grad_norm": 2.711665391921997,
      "learning_rate": 1.2427126710291495e-05,
      "loss": 0.9778,
      "step": 3820
    },
    {
      "epoch": 1.1392028554431886,
      "grad_norm": 1.8573246002197266,
      "learning_rate": 1.2407297243704146e-05,
      "loss": 0.9992,
      "step": 3830
    },
    {
      "epoch": 1.1421772754312909,
      "grad_norm": 2.408982038497925,
      "learning_rate": 1.2387467777116797e-05,
      "loss": 0.9817,
      "step": 3840
    },
    {
      "epoch": 1.1451516954193932,
      "grad_norm": 2.57694673538208,
      "learning_rate": 1.2367638310529448e-05,
      "loss": 0.9002,
      "step": 3850
    },
    {
      "epoch": 1.1481261154074955,
      "grad_norm": 1.9325779676437378,
      "learning_rate": 1.23478088439421e-05,
      "loss": 0.927,
      "step": 3860
    },
    {
      "epoch": 1.1511005353955979,
      "grad_norm": 1.8228466510772705,
      "learning_rate": 1.232797937735475e-05,
      "loss": 0.9286,
      "step": 3870
    },
    {
      "epoch": 1.1540749553837002,
      "grad_norm": 8.077204704284668,
      "learning_rate": 1.2308149910767402e-05,
      "loss": 0.8951,
      "step": 3880
    },
    {
      "epoch": 1.1570493753718025,
      "grad_norm": 1.7447963953018188,
      "learning_rate": 1.2288320444180053e-05,
      "loss": 1.1357,
      "step": 3890
    },
    {
      "epoch": 1.1600237953599049,
      "grad_norm": 2.08679461479187,
      "learning_rate": 1.2268490977592704e-05,
      "loss": 0.9882,
      "step": 3900
    },
    {
      "epoch": 1.1629982153480072,
      "grad_norm": 2.158597946166992,
      "learning_rate": 1.2248661511005355e-05,
      "loss": 0.9096,
      "step": 3910
    },
    {
      "epoch": 1.1659726353361095,
      "grad_norm": 2.4603521823883057,
      "learning_rate": 1.2228832044418006e-05,
      "loss": 0.986,
      "step": 3920
    },
    {
      "epoch": 1.1689470553242118,
      "grad_norm": 2.876965045928955,
      "learning_rate": 1.2209002577830657e-05,
      "loss": 0.9816,
      "step": 3930
    },
    {
      "epoch": 1.1719214753123142,
      "grad_norm": 2.095338821411133,
      "learning_rate": 1.2189173111243308e-05,
      "loss": 1.0991,
      "step": 3940
    },
    {
      "epoch": 1.1748958953004165,
      "grad_norm": 3.2862977981567383,
      "learning_rate": 1.216934364465596e-05,
      "loss": 1.085,
      "step": 3950
    },
    {
      "epoch": 1.1778703152885188,
      "grad_norm": 2.0990495681762695,
      "learning_rate": 1.214951417806861e-05,
      "loss": 0.9024,
      "step": 3960
    },
    {
      "epoch": 1.1808447352766211,
      "grad_norm": 3.3444840908050537,
      "learning_rate": 1.2129684711481261e-05,
      "loss": 1.0742,
      "step": 3970
    },
    {
      "epoch": 1.1838191552647235,
      "grad_norm": 2.852485179901123,
      "learning_rate": 1.2109855244893913e-05,
      "loss": 1.0575,
      "step": 3980
    },
    {
      "epoch": 1.1867935752528256,
      "grad_norm": 1.6780016422271729,
      "learning_rate": 1.2090025778306564e-05,
      "loss": 1.0545,
      "step": 3990
    },
    {
      "epoch": 1.1897679952409281,
      "grad_norm": 4.192241191864014,
      "learning_rate": 1.2070196311719215e-05,
      "loss": 1.0606,
      "step": 4000
    },
    {
      "epoch": 1.1927424152290302,
      "grad_norm": 2.384012222290039,
      "learning_rate": 1.2050366845131868e-05,
      "loss": 0.9522,
      "step": 4010
    },
    {
      "epoch": 1.1957168352171326,
      "grad_norm": 2.155952215194702,
      "learning_rate": 1.2030537378544519e-05,
      "loss": 0.9762,
      "step": 4020
    },
    {
      "epoch": 1.198691255205235,
      "grad_norm": 1.7284510135650635,
      "learning_rate": 1.201070791195717e-05,
      "loss": 0.896,
      "step": 4030
    },
    {
      "epoch": 1.2016656751933372,
      "grad_norm": 2.2615063190460205,
      "learning_rate": 1.199087844536982e-05,
      "loss": 0.975,
      "step": 4040
    },
    {
      "epoch": 1.2046400951814396,
      "grad_norm": 4.617161750793457,
      "learning_rate": 1.1971048978782472e-05,
      "loss": 0.9717,
      "step": 4050
    },
    {
      "epoch": 1.2076145151695419,
      "grad_norm": 5.370487213134766,
      "learning_rate": 1.1951219512195123e-05,
      "loss": 0.9708,
      "step": 4060
    },
    {
      "epoch": 1.2105889351576442,
      "grad_norm": 2.0268101692199707,
      "learning_rate": 1.1931390045607774e-05,
      "loss": 0.9128,
      "step": 4070
    },
    {
      "epoch": 1.2135633551457465,
      "grad_norm": 7.18986177444458,
      "learning_rate": 1.1911560579020425e-05,
      "loss": 0.9249,
      "step": 4080
    },
    {
      "epoch": 1.2165377751338489,
      "grad_norm": 2.6054046154022217,
      "learning_rate": 1.1891731112433076e-05,
      "loss": 0.9308,
      "step": 4090
    },
    {
      "epoch": 1.2195121951219512,
      "grad_norm": 2.528301477432251,
      "learning_rate": 1.1871901645845727e-05,
      "loss": 1.0292,
      "step": 4100
    },
    {
      "epoch": 1.2224866151100535,
      "grad_norm": 2.060906410217285,
      "learning_rate": 1.1852072179258378e-05,
      "loss": 0.8462,
      "step": 4110
    },
    {
      "epoch": 1.2254610350981558,
      "grad_norm": 1.7874221801757812,
      "learning_rate": 1.183224271267103e-05,
      "loss": 0.8993,
      "step": 4120
    },
    {
      "epoch": 1.2284354550862582,
      "grad_norm": 2.452101945877075,
      "learning_rate": 1.181241324608368e-05,
      "loss": 1.1325,
      "step": 4130
    },
    {
      "epoch": 1.2314098750743605,
      "grad_norm": 2.044489622116089,
      "learning_rate": 1.1792583779496332e-05,
      "loss": 0.9585,
      "step": 4140
    },
    {
      "epoch": 1.2343842950624628,
      "grad_norm": 3.345743417739868,
      "learning_rate": 1.1772754312908983e-05,
      "loss": 1.1531,
      "step": 4150
    },
    {
      "epoch": 1.2373587150505652,
      "grad_norm": 2.355480670928955,
      "learning_rate": 1.1752924846321634e-05,
      "loss": 1.005,
      "step": 4160
    },
    {
      "epoch": 1.2403331350386675,
      "grad_norm": 3.1936066150665283,
      "learning_rate": 1.1733095379734285e-05,
      "loss": 0.9719,
      "step": 4170
    },
    {
      "epoch": 1.2433075550267698,
      "grad_norm": 1.7337291240692139,
      "learning_rate": 1.1713265913146938e-05,
      "loss": 1.0038,
      "step": 4180
    },
    {
      "epoch": 1.2462819750148721,
      "grad_norm": 1.8889919519424438,
      "learning_rate": 1.1693436446559589e-05,
      "loss": 0.9767,
      "step": 4190
    },
    {
      "epoch": 1.2492563950029745,
      "grad_norm": 2.1152963638305664,
      "learning_rate": 1.167360697997224e-05,
      "loss": 0.9233,
      "step": 4200
    },
    {
      "epoch": 1.2522308149910768,
      "grad_norm": 2.2388548851013184,
      "learning_rate": 1.1653777513384891e-05,
      "loss": 0.8992,
      "step": 4210
    },
    {
      "epoch": 1.2552052349791791,
      "grad_norm": 2.4411463737487793,
      "learning_rate": 1.1633948046797542e-05,
      "loss": 0.9375,
      "step": 4220
    },
    {
      "epoch": 1.2581796549672815,
      "grad_norm": 2.2517142295837402,
      "learning_rate": 1.1614118580210193e-05,
      "loss": 1.0001,
      "step": 4230
    },
    {
      "epoch": 1.2611540749553836,
      "grad_norm": 4.086191654205322,
      "learning_rate": 1.1594289113622846e-05,
      "loss": 0.9988,
      "step": 4240
    },
    {
      "epoch": 1.2641284949434861,
      "grad_norm": 2.129183292388916,
      "learning_rate": 1.1574459647035497e-05,
      "loss": 0.9099,
      "step": 4250
    },
    {
      "epoch": 1.2671029149315882,
      "grad_norm": 2.171566963195801,
      "learning_rate": 1.1554630180448148e-05,
      "loss": 1.0687,
      "step": 4260
    },
    {
      "epoch": 1.2700773349196908,
      "grad_norm": 2.1937763690948486,
      "learning_rate": 1.15348007138608e-05,
      "loss": 0.9621,
      "step": 4270
    },
    {
      "epoch": 1.2730517549077929,
      "grad_norm": 3.100571393966675,
      "learning_rate": 1.151497124727345e-05,
      "loss": 0.893,
      "step": 4280
    },
    {
      "epoch": 1.2760261748958954,
      "grad_norm": 2.5700550079345703,
      "learning_rate": 1.1495141780686102e-05,
      "loss": 0.9897,
      "step": 4290
    },
    {
      "epoch": 1.2790005948839975,
      "grad_norm": 1.898512601852417,
      "learning_rate": 1.1475312314098753e-05,
      "loss": 0.9419,
      "step": 4300
    },
    {
      "epoch": 1.2819750148720999,
      "grad_norm": 2.010921001434326,
      "learning_rate": 1.1455482847511404e-05,
      "loss": 0.9944,
      "step": 4310
    },
    {
      "epoch": 1.2849494348602022,
      "grad_norm": 1.6894621849060059,
      "learning_rate": 1.1435653380924055e-05,
      "loss": 0.8922,
      "step": 4320
    },
    {
      "epoch": 1.2879238548483045,
      "grad_norm": 2.3370847702026367,
      "learning_rate": 1.1415823914336706e-05,
      "loss": 1.0482,
      "step": 4330
    },
    {
      "epoch": 1.2908982748364068,
      "grad_norm": 2.498735189437866,
      "learning_rate": 1.1395994447749357e-05,
      "loss": 1.046,
      "step": 4340
    },
    {
      "epoch": 1.2938726948245092,
      "grad_norm": 3.2094192504882812,
      "learning_rate": 1.1376164981162008e-05,
      "loss": 1.1706,
      "step": 4350
    },
    {
      "epoch": 1.2968471148126115,
      "grad_norm": 2.1121299266815186,
      "learning_rate": 1.135633551457466e-05,
      "loss": 0.9001,
      "step": 4360
    },
    {
      "epoch": 1.2998215348007138,
      "grad_norm": 2.1064391136169434,
      "learning_rate": 1.133650604798731e-05,
      "loss": 1.011,
      "step": 4370
    },
    {
      "epoch": 1.3027959547888162,
      "grad_norm": 2.234314441680908,
      "learning_rate": 1.1316676581399961e-05,
      "loss": 0.9015,
      "step": 4380
    },
    {
      "epoch": 1.3057703747769185,
      "grad_norm": 1.818114161491394,
      "learning_rate": 1.1296847114812612e-05,
      "loss": 0.9723,
      "step": 4390
    },
    {
      "epoch": 1.3087447947650208,
      "grad_norm": 2.0157649517059326,
      "learning_rate": 1.1277017648225264e-05,
      "loss": 0.9161,
      "step": 4400
    },
    {
      "epoch": 1.3117192147531231,
      "grad_norm": 2.104449987411499,
      "learning_rate": 1.1257188181637915e-05,
      "loss": 1.1273,
      "step": 4410
    },
    {
      "epoch": 1.3146936347412255,
      "grad_norm": 1.9420799016952515,
      "learning_rate": 1.1237358715050566e-05,
      "loss": 0.8845,
      "step": 4420
    },
    {
      "epoch": 1.3176680547293278,
      "grad_norm": 5.677300453186035,
      "learning_rate": 1.1217529248463217e-05,
      "loss": 0.9527,
      "step": 4430
    },
    {
      "epoch": 1.3206424747174301,
      "grad_norm": 2.0025525093078613,
      "learning_rate": 1.1197699781875868e-05,
      "loss": 1.0623,
      "step": 4440
    },
    {
      "epoch": 1.3236168947055325,
      "grad_norm": 2.0340163707733154,
      "learning_rate": 1.1177870315288519e-05,
      "loss": 1.0993,
      "step": 4450
    },
    {
      "epoch": 1.3265913146936348,
      "grad_norm": 3.2343392372131348,
      "learning_rate": 1.115804084870117e-05,
      "loss": 1.0821,
      "step": 4460
    },
    {
      "epoch": 1.329565734681737,
      "grad_norm": 2.0781054496765137,
      "learning_rate": 1.1138211382113821e-05,
      "loss": 0.9446,
      "step": 4470
    },
    {
      "epoch": 1.3325401546698394,
      "grad_norm": 1.6292167901992798,
      "learning_rate": 1.1118381915526472e-05,
      "loss": 1.0544,
      "step": 4480
    },
    {
      "epoch": 1.3355145746579418,
      "grad_norm": 2.607245445251465,
      "learning_rate": 1.1098552448939123e-05,
      "loss": 0.8355,
      "step": 4490
    },
    {
      "epoch": 1.338488994646044,
      "grad_norm": 1.9325217008590698,
      "learning_rate": 1.1078722982351775e-05,
      "loss": 0.9669,
      "step": 4500
    },
    {
      "epoch": 1.3414634146341464,
      "grad_norm": 3.326625108718872,
      "learning_rate": 1.1058893515764426e-05,
      "loss": 1.0669,
      "step": 4510
    },
    {
      "epoch": 1.3444378346222488,
      "grad_norm": 1.957566738128662,
      "learning_rate": 1.1039064049177077e-05,
      "loss": 0.8478,
      "step": 4520
    },
    {
      "epoch": 1.3474122546103509,
      "grad_norm": 2.174494981765747,
      "learning_rate": 1.1019234582589728e-05,
      "loss": 1.0075,
      "step": 4530
    },
    {
      "epoch": 1.3503866745984534,
      "grad_norm": 1.8105746507644653,
      "learning_rate": 1.0999405116002379e-05,
      "loss": 0.9594,
      "step": 4540
    },
    {
      "epoch": 1.3533610945865555,
      "grad_norm": 1.7986018657684326,
      "learning_rate": 1.097957564941503e-05,
      "loss": 0.8761,
      "step": 4550
    },
    {
      "epoch": 1.356335514574658,
      "grad_norm": 2.9492223262786865,
      "learning_rate": 1.0959746182827681e-05,
      "loss": 0.98,
      "step": 4560
    },
    {
      "epoch": 1.3593099345627602,
      "grad_norm": 1.7622472047805786,
      "learning_rate": 1.0939916716240332e-05,
      "loss": 0.9155,
      "step": 4570
    },
    {
      "epoch": 1.3622843545508627,
      "grad_norm": 2.4196295738220215,
      "learning_rate": 1.0920087249652985e-05,
      "loss": 0.8895,
      "step": 4580
    },
    {
      "epoch": 1.3652587745389648,
      "grad_norm": 2.374565601348877,
      "learning_rate": 1.0900257783065638e-05,
      "loss": 0.9713,
      "step": 4590
    },
    {
      "epoch": 1.3682331945270672,
      "grad_norm": 2.6737987995147705,
      "learning_rate": 1.0880428316478289e-05,
      "loss": 1.063,
      "step": 4600
    },
    {
      "epoch": 1.3712076145151695,
      "grad_norm": 2.077803134918213,
      "learning_rate": 1.086059884989094e-05,
      "loss": 0.8888,
      "step": 4610
    },
    {
      "epoch": 1.3741820345032718,
      "grad_norm": 2.179300546646118,
      "learning_rate": 1.0840769383303591e-05,
      "loss": 1.0149,
      "step": 4620
    },
    {
      "epoch": 1.3771564544913741,
      "grad_norm": 2.517874240875244,
      "learning_rate": 1.0820939916716242e-05,
      "loss": 0.9349,
      "step": 4630
    },
    {
      "epoch": 1.3801308744794765,
      "grad_norm": 3.0415220260620117,
      "learning_rate": 1.0801110450128893e-05,
      "loss": 1.0266,
      "step": 4640
    },
    {
      "epoch": 1.3831052944675788,
      "grad_norm": 2.2428321838378906,
      "learning_rate": 1.0781280983541544e-05,
      "loss": 0.8983,
      "step": 4650
    },
    {
      "epoch": 1.3860797144556811,
      "grad_norm": 3.284655809402466,
      "learning_rate": 1.0761451516954195e-05,
      "loss": 1.0169,
      "step": 4660
    },
    {
      "epoch": 1.3890541344437835,
      "grad_norm": 1.9388266801834106,
      "learning_rate": 1.0741622050366847e-05,
      "loss": 1.0055,
      "step": 4670
    },
    {
      "epoch": 1.3920285544318858,
      "grad_norm": 2.7885806560516357,
      "learning_rate": 1.0721792583779498e-05,
      "loss": 0.8927,
      "step": 4680
    },
    {
      "epoch": 1.395002974419988,
      "grad_norm": 2.7265586853027344,
      "learning_rate": 1.0701963117192149e-05,
      "loss": 0.9421,
      "step": 4690
    },
    {
      "epoch": 1.3979773944080904,
      "grad_norm": 1.9524320363998413,
      "learning_rate": 1.06821336506048e-05,
      "loss": 0.9423,
      "step": 4700
    },
    {
      "epoch": 1.4009518143961928,
      "grad_norm": 8.586723327636719,
      "learning_rate": 1.0662304184017451e-05,
      "loss": 1.068,
      "step": 4710
    },
    {
      "epoch": 1.403926234384295,
      "grad_norm": 2.778289318084717,
      "learning_rate": 1.0642474717430102e-05,
      "loss": 1.0831,
      "step": 4720
    },
    {
      "epoch": 1.4069006543723974,
      "grad_norm": 2.1474833488464355,
      "learning_rate": 1.0622645250842753e-05,
      "loss": 0.9549,
      "step": 4730
    },
    {
      "epoch": 1.4098750743604997,
      "grad_norm": 7.097409725189209,
      "learning_rate": 1.0602815784255404e-05,
      "loss": 0.9334,
      "step": 4740
    },
    {
      "epoch": 1.412849494348602,
      "grad_norm": 2.947734832763672,
      "learning_rate": 1.0582986317668055e-05,
      "loss": 1.0861,
      "step": 4750
    },
    {
      "epoch": 1.4158239143367044,
      "grad_norm": 2.3749964237213135,
      "learning_rate": 1.0563156851080706e-05,
      "loss": 0.976,
      "step": 4760
    },
    {
      "epoch": 1.4187983343248067,
      "grad_norm": 2.255152702331543,
      "learning_rate": 1.0543327384493357e-05,
      "loss": 1.0204,
      "step": 4770
    },
    {
      "epoch": 1.421772754312909,
      "grad_norm": 2.143341541290283,
      "learning_rate": 1.0523497917906009e-05,
      "loss": 1.0353,
      "step": 4780
    },
    {
      "epoch": 1.4247471743010114,
      "grad_norm": 2.6695749759674072,
      "learning_rate": 1.050366845131866e-05,
      "loss": 0.9314,
      "step": 4790
    },
    {
      "epoch": 1.4277215942891135,
      "grad_norm": 2.3598549365997314,
      "learning_rate": 1.048383898473131e-05,
      "loss": 0.9008,
      "step": 4800
    },
    {
      "epoch": 1.430696014277216,
      "grad_norm": 6.043622016906738,
      "learning_rate": 1.0464009518143964e-05,
      "loss": 0.9283,
      "step": 4810
    },
    {
      "epoch": 1.4336704342653181,
      "grad_norm": 2.0088846683502197,
      "learning_rate": 1.0444180051556615e-05,
      "loss": 0.8871,
      "step": 4820
    },
    {
      "epoch": 1.4366448542534207,
      "grad_norm": 5.859786510467529,
      "learning_rate": 1.0424350584969266e-05,
      "loss": 0.964,
      "step": 4830
    },
    {
      "epoch": 1.4396192742415228,
      "grad_norm": 2.8884341716766357,
      "learning_rate": 1.0404521118381917e-05,
      "loss": 1.0119,
      "step": 4840
    },
    {
      "epoch": 1.4425936942296254,
      "grad_norm": 2.2336933612823486,
      "learning_rate": 1.0384691651794568e-05,
      "loss": 1.0217,
      "step": 4850
    },
    {
      "epoch": 1.4455681142177275,
      "grad_norm": 3.3629086017608643,
      "learning_rate": 1.0364862185207219e-05,
      "loss": 1.0059,
      "step": 4860
    },
    {
      "epoch": 1.4485425342058298,
      "grad_norm": 2.1546435356140137,
      "learning_rate": 1.034503271861987e-05,
      "loss": 1.0414,
      "step": 4870
    },
    {
      "epoch": 1.4515169541939321,
      "grad_norm": 2.180363893508911,
      "learning_rate": 1.0325203252032521e-05,
      "loss": 0.9024,
      "step": 4880
    },
    {
      "epoch": 1.4544913741820344,
      "grad_norm": 2.830793857574463,
      "learning_rate": 1.0305373785445172e-05,
      "loss": 0.8825,
      "step": 4890
    },
    {
      "epoch": 1.4574657941701368,
      "grad_norm": 3.6156301498413086,
      "learning_rate": 1.0285544318857823e-05,
      "loss": 0.892,
      "step": 4900
    },
    {
      "epoch": 1.460440214158239,
      "grad_norm": 2.3766157627105713,
      "learning_rate": 1.0265714852270474e-05,
      "loss": 0.9081,
      "step": 4910
    },
    {
      "epoch": 1.4634146341463414,
      "grad_norm": 3.333779811859131,
      "learning_rate": 1.0245885385683126e-05,
      "loss": 1.0296,
      "step": 4920
    },
    {
      "epoch": 1.4663890541344438,
      "grad_norm": 1.8320388793945312,
      "learning_rate": 1.0226055919095777e-05,
      "loss": 0.9428,
      "step": 4930
    },
    {
      "epoch": 1.469363474122546,
      "grad_norm": 2.3891632556915283,
      "learning_rate": 1.0206226452508428e-05,
      "loss": 0.9284,
      "step": 4940
    },
    {
      "epoch": 1.4723378941106484,
      "grad_norm": 1.620604395866394,
      "learning_rate": 1.0186396985921079e-05,
      "loss": 0.8766,
      "step": 4950
    },
    {
      "epoch": 1.4753123140987507,
      "grad_norm": 2.065389394760132,
      "learning_rate": 1.016656751933373e-05,
      "loss": 0.992,
      "step": 4960
    },
    {
      "epoch": 1.478286734086853,
      "grad_norm": 2.0230751037597656,
      "learning_rate": 1.0146738052746381e-05,
      "loss": 1.0576,
      "step": 4970
    },
    {
      "epoch": 1.4812611540749554,
      "grad_norm": 2.1217973232269287,
      "learning_rate": 1.0126908586159032e-05,
      "loss": 0.915,
      "step": 4980
    },
    {
      "epoch": 1.4842355740630577,
      "grad_norm": 1.8332499265670776,
      "learning_rate": 1.0107079119571683e-05,
      "loss": 0.91,
      "step": 4990
    },
    {
      "epoch": 1.48720999405116,
      "grad_norm": 3.039701223373413,
      "learning_rate": 1.0087249652984334e-05,
      "loss": 0.8318,
      "step": 5000
    },
    {
      "epoch": 1.4901844140392624,
      "grad_norm": 1.6759006977081299,
      "learning_rate": 1.0067420186396987e-05,
      "loss": 0.965,
      "step": 5010
    },
    {
      "epoch": 1.4931588340273647,
      "grad_norm": 1.8956881761550903,
      "learning_rate": 1.0047590719809638e-05,
      "loss": 0.9593,
      "step": 5020
    },
    {
      "epoch": 1.496133254015467,
      "grad_norm": 1.592418909072876,
      "learning_rate": 1.002776125322229e-05,
      "loss": 0.9937,
      "step": 5030
    },
    {
      "epoch": 1.4991076740035694,
      "grad_norm": 1.6711852550506592,
      "learning_rate": 1.0007931786634942e-05,
      "loss": 0.9621,
      "step": 5040
    },
    {
      "epoch": 1.5020820939916715,
      "grad_norm": 2.908905506134033,
      "learning_rate": 9.988102320047591e-06,
      "loss": 0.917,
      "step": 5050
    },
    {
      "epoch": 1.505056513979774,
      "grad_norm": 3.0222597122192383,
      "learning_rate": 9.968272853460243e-06,
      "loss": 0.9848,
      "step": 5060
    },
    {
      "epoch": 1.5080309339678761,
      "grad_norm": 2.238077402114868,
      "learning_rate": 9.948443386872894e-06,
      "loss": 1.024,
      "step": 5070
    },
    {
      "epoch": 1.5110053539559787,
      "grad_norm": 5.5751848220825195,
      "learning_rate": 9.928613920285545e-06,
      "loss": 0.8789,
      "step": 5080
    },
    {
      "epoch": 1.5139797739440808,
      "grad_norm": 1.8254358768463135,
      "learning_rate": 9.908784453698196e-06,
      "loss": 0.9182,
      "step": 5090
    },
    {
      "epoch": 1.5169541939321833,
      "grad_norm": 2.535499095916748,
      "learning_rate": 9.888954987110847e-06,
      "loss": 0.885,
      "step": 5100
    },
    {
      "epoch": 1.5199286139202854,
      "grad_norm": 2.3101959228515625,
      "learning_rate": 9.8691255205235e-06,
      "loss": 0.9502,
      "step": 5110
    },
    {
      "epoch": 1.522903033908388,
      "grad_norm": 2.0638699531555176,
      "learning_rate": 9.84929605393615e-06,
      "loss": 0.9922,
      "step": 5120
    },
    {
      "epoch": 1.52587745389649,
      "grad_norm": 2.2800979614257812,
      "learning_rate": 9.829466587348802e-06,
      "loss": 0.9445,
      "step": 5130
    },
    {
      "epoch": 1.5288518738845926,
      "grad_norm": 1.787011981010437,
      "learning_rate": 9.809637120761453e-06,
      "loss": 0.9223,
      "step": 5140
    },
    {
      "epoch": 1.5318262938726948,
      "grad_norm": 2.342102289199829,
      "learning_rate": 9.789807654174104e-06,
      "loss": 0.8685,
      "step": 5150
    },
    {
      "epoch": 1.5348007138607973,
      "grad_norm": 1.8339080810546875,
      "learning_rate": 9.769978187586755e-06,
      "loss": 0.9263,
      "step": 5160
    },
    {
      "epoch": 1.5377751338488994,
      "grad_norm": 2.008483648300171,
      "learning_rate": 9.750148720999406e-06,
      "loss": 1.018,
      "step": 5170
    },
    {
      "epoch": 1.5407495538370017,
      "grad_norm": 1.6090691089630127,
      "learning_rate": 9.730319254412057e-06,
      "loss": 1.0887,
      "step": 5180
    },
    {
      "epoch": 1.543723973825104,
      "grad_norm": 2.912973403930664,
      "learning_rate": 9.710489787824708e-06,
      "loss": 1.0189,
      "step": 5190
    },
    {
      "epoch": 1.5466983938132064,
      "grad_norm": 1.993403673171997,
      "learning_rate": 9.69066032123736e-06,
      "loss": 0.9128,
      "step": 5200
    },
    {
      "epoch": 1.5496728138013087,
      "grad_norm": 2.0426414012908936,
      "learning_rate": 9.67083085465001e-06,
      "loss": 0.9044,
      "step": 5210
    },
    {
      "epoch": 1.552647233789411,
      "grad_norm": 1.9683917760849,
      "learning_rate": 9.651001388062662e-06,
      "loss": 0.967,
      "step": 5220
    },
    {
      "epoch": 1.5556216537775134,
      "grad_norm": 3.2990283966064453,
      "learning_rate": 9.631171921475313e-06,
      "loss": 0.9499,
      "step": 5230
    },
    {
      "epoch": 1.5585960737656157,
      "grad_norm": 1.6027984619140625,
      "learning_rate": 9.611342454887964e-06,
      "loss": 0.8487,
      "step": 5240
    },
    {
      "epoch": 1.561570493753718,
      "grad_norm": 2.575690507888794,
      "learning_rate": 9.591512988300615e-06,
      "loss": 0.9442,
      "step": 5250
    },
    {
      "epoch": 1.5645449137418204,
      "grad_norm": 2.1755945682525635,
      "learning_rate": 9.571683521713266e-06,
      "loss": 0.8202,
      "step": 5260
    },
    {
      "epoch": 1.5675193337299227,
      "grad_norm": 1.9880266189575195,
      "learning_rate": 9.551854055125917e-06,
      "loss": 1.0293,
      "step": 5270
    },
    {
      "epoch": 1.570493753718025,
      "grad_norm": 2.3361849784851074,
      "learning_rate": 9.532024588538568e-06,
      "loss": 1.0027,
      "step": 5280
    },
    {
      "epoch": 1.5734681737061273,
      "grad_norm": 2.1314916610717773,
      "learning_rate": 9.51219512195122e-06,
      "loss": 1.0444,
      "step": 5290
    },
    {
      "epoch": 1.5764425936942297,
      "grad_norm": 2.2029247283935547,
      "learning_rate": 9.49236565536387e-06,
      "loss": 0.8896,
      "step": 5300
    },
    {
      "epoch": 1.579417013682332,
      "grad_norm": 1.835067629814148,
      "learning_rate": 9.472536188776522e-06,
      "loss": 0.9015,
      "step": 5310
    },
    {
      "epoch": 1.5823914336704341,
      "grad_norm": 1.6770776510238647,
      "learning_rate": 9.452706722189174e-06,
      "loss": 0.917,
      "step": 5320
    },
    {
      "epoch": 1.5853658536585367,
      "grad_norm": 2.523510217666626,
      "learning_rate": 9.432877255601825e-06,
      "loss": 0.9234,
      "step": 5330
    },
    {
      "epoch": 1.5883402736466388,
      "grad_norm": 1.7482106685638428,
      "learning_rate": 9.413047789014477e-06,
      "loss": 1.0043,
      "step": 5340
    },
    {
      "epoch": 1.5913146936347413,
      "grad_norm": 2.1277012825012207,
      "learning_rate": 9.393218322427128e-06,
      "loss": 1.0179,
      "step": 5350
    },
    {
      "epoch": 1.5942891136228434,
      "grad_norm": 3.268005609512329,
      "learning_rate": 9.373388855839779e-06,
      "loss": 0.9932,
      "step": 5360
    },
    {
      "epoch": 1.597263533610946,
      "grad_norm": 2.5311810970306396,
      "learning_rate": 9.35355938925243e-06,
      "loss": 0.9372,
      "step": 5370
    },
    {
      "epoch": 1.600237953599048,
      "grad_norm": 3.735252618789673,
      "learning_rate": 9.333729922665081e-06,
      "loss": 0.8645,
      "step": 5380
    },
    {
      "epoch": 1.6032123735871506,
      "grad_norm": 1.8741289377212524,
      "learning_rate": 9.313900456077732e-06,
      "loss": 0.8547,
      "step": 5390
    },
    {
      "epoch": 1.6061867935752527,
      "grad_norm": 2.741715669631958,
      "learning_rate": 9.294070989490383e-06,
      "loss": 0.9382,
      "step": 5400
    },
    {
      "epoch": 1.6091612135633553,
      "grad_norm": 1.60530424118042,
      "learning_rate": 9.274241522903034e-06,
      "loss": 0.9396,
      "step": 5410
    },
    {
      "epoch": 1.6121356335514574,
      "grad_norm": 2.3212172985076904,
      "learning_rate": 9.254412056315685e-06,
      "loss": 0.966,
      "step": 5420
    },
    {
      "epoch": 1.61511005353956,
      "grad_norm": 1.5596733093261719,
      "learning_rate": 9.234582589728336e-06,
      "loss": 0.8702,
      "step": 5430
    },
    {
      "epoch": 1.618084473527662,
      "grad_norm": 1.9577252864837646,
      "learning_rate": 9.214753123140988e-06,
      "loss": 0.8695,
      "step": 5440
    },
    {
      "epoch": 1.6210588935157644,
      "grad_norm": 2.0002522468566895,
      "learning_rate": 9.194923656553639e-06,
      "loss": 0.9154,
      "step": 5450
    },
    {
      "epoch": 1.6240333135038667,
      "grad_norm": 4.249907970428467,
      "learning_rate": 9.17509418996629e-06,
      "loss": 0.9293,
      "step": 5460
    },
    {
      "epoch": 1.627007733491969,
      "grad_norm": 2.262089729309082,
      "learning_rate": 9.15526472337894e-06,
      "loss": 0.8955,
      "step": 5470
    },
    {
      "epoch": 1.6299821534800714,
      "grad_norm": 5.06208610534668,
      "learning_rate": 9.135435256791592e-06,
      "loss": 0.9466,
      "step": 5480
    },
    {
      "epoch": 1.6329565734681737,
      "grad_norm": 1.983320713043213,
      "learning_rate": 9.115605790204245e-06,
      "loss": 0.8841,
      "step": 5490
    },
    {
      "epoch": 1.635930993456276,
      "grad_norm": 2.2005064487457275,
      "learning_rate": 9.095776323616896e-06,
      "loss": 0.9281,
      "step": 5500
    },
    {
      "epoch": 1.6389054134443783,
      "grad_norm": 1.9587833881378174,
      "learning_rate": 9.075946857029547e-06,
      "loss": 0.9427,
      "step": 5510
    },
    {
      "epoch": 1.6418798334324807,
      "grad_norm": 4.282254695892334,
      "learning_rate": 9.056117390442198e-06,
      "loss": 0.9559,
      "step": 5520
    },
    {
      "epoch": 1.644854253420583,
      "grad_norm": 2.2592613697052,
      "learning_rate": 9.036287923854849e-06,
      "loss": 0.7955,
      "step": 5530
    },
    {
      "epoch": 1.6478286734086853,
      "grad_norm": 1.5266762971878052,
      "learning_rate": 9.0164584572675e-06,
      "loss": 0.9402,
      "step": 5540
    },
    {
      "epoch": 1.6508030933967877,
      "grad_norm": 1.9437406063079834,
      "learning_rate": 8.996628990680151e-06,
      "loss": 0.9501,
      "step": 5550
    },
    {
      "epoch": 1.65377751338489,
      "grad_norm": 1.8853552341461182,
      "learning_rate": 8.976799524092802e-06,
      "loss": 0.9274,
      "step": 5560
    },
    {
      "epoch": 1.6567519333729923,
      "grad_norm": 2.2992374897003174,
      "learning_rate": 8.956970057505453e-06,
      "loss": 1.005,
      "step": 5570
    },
    {
      "epoch": 1.6597263533610946,
      "grad_norm": 1.8113545179367065,
      "learning_rate": 8.937140590918105e-06,
      "loss": 0.9182,
      "step": 5580
    },
    {
      "epoch": 1.6627007733491967,
      "grad_norm": 2.923424243927002,
      "learning_rate": 8.917311124330756e-06,
      "loss": 0.8466,
      "step": 5590
    },
    {
      "epoch": 1.6656751933372993,
      "grad_norm": 2.6633269786834717,
      "learning_rate": 8.897481657743407e-06,
      "loss": 0.9711,
      "step": 5600
    },
    {
      "epoch": 1.6686496133254014,
      "grad_norm": 2.2023210525512695,
      "learning_rate": 8.87765219115606e-06,
      "loss": 0.8034,
      "step": 5610
    },
    {
      "epoch": 1.671624033313504,
      "grad_norm": 2.8525261878967285,
      "learning_rate": 8.85782272456871e-06,
      "loss": 1.0122,
      "step": 5620
    },
    {
      "epoch": 1.674598453301606,
      "grad_norm": 1.7569255828857422,
      "learning_rate": 8.837993257981362e-06,
      "loss": 0.9327,
      "step": 5630
    },
    {
      "epoch": 1.6775728732897086,
      "grad_norm": 5.573215007781982,
      "learning_rate": 8.818163791394013e-06,
      "loss": 0.9389,
      "step": 5640
    },
    {
      "epoch": 1.6805472932778107,
      "grad_norm": 5.177669048309326,
      "learning_rate": 8.798334324806664e-06,
      "loss": 1.0392,
      "step": 5650
    },
    {
      "epoch": 1.6835217132659133,
      "grad_norm": 1.8438141345977783,
      "learning_rate": 8.778504858219315e-06,
      "loss": 0.8696,
      "step": 5660
    },
    {
      "epoch": 1.6864961332540154,
      "grad_norm": 3.9441463947296143,
      "learning_rate": 8.758675391631966e-06,
      "loss": 1.0036,
      "step": 5670
    },
    {
      "epoch": 1.689470553242118,
      "grad_norm": 1.757725715637207,
      "learning_rate": 8.738845925044617e-06,
      "loss": 0.9104,
      "step": 5680
    },
    {
      "epoch": 1.69244497323022,
      "grad_norm": 2.7551960945129395,
      "learning_rate": 8.719016458457268e-06,
      "loss": 0.958,
      "step": 5690
    },
    {
      "epoch": 1.6954193932183226,
      "grad_norm": 2.5900769233703613,
      "learning_rate": 8.69918699186992e-06,
      "loss": 0.8809,
      "step": 5700
    },
    {
      "epoch": 1.6983938132064247,
      "grad_norm": 5.644659519195557,
      "learning_rate": 8.67935752528257e-06,
      "loss": 0.9498,
      "step": 5710
    },
    {
      "epoch": 1.701368233194527,
      "grad_norm": 4.4540205001831055,
      "learning_rate": 8.659528058695222e-06,
      "loss": 0.8711,
      "step": 5720
    },
    {
      "epoch": 1.7043426531826293,
      "grad_norm": 4.720098972320557,
      "learning_rate": 8.639698592107874e-06,
      "loss": 0.8557,
      "step": 5730
    },
    {
      "epoch": 1.7073170731707317,
      "grad_norm": 2.128326416015625,
      "learning_rate": 8.619869125520525e-06,
      "loss": 0.9569,
      "step": 5740
    },
    {
      "epoch": 1.710291493158834,
      "grad_norm": 3.5595510005950928,
      "learning_rate": 8.600039658933177e-06,
      "loss": 0.9446,
      "step": 5750
    },
    {
      "epoch": 1.7132659131469363,
      "grad_norm": 2.2753734588623047,
      "learning_rate": 8.580210192345828e-06,
      "loss": 0.9743,
      "step": 5760
    },
    {
      "epoch": 1.7162403331350387,
      "grad_norm": 6.064225196838379,
      "learning_rate": 8.560380725758479e-06,
      "loss": 1.019,
      "step": 5770
    },
    {
      "epoch": 1.719214753123141,
      "grad_norm": 1.9843639135360718,
      "learning_rate": 8.54055125917113e-06,
      "loss": 0.9273,
      "step": 5780
    },
    {
      "epoch": 1.7221891731112433,
      "grad_norm": 2.3234448432922363,
      "learning_rate": 8.520721792583781e-06,
      "loss": 1.0742,
      "step": 5790
    },
    {
      "epoch": 1.7251635930993456,
      "grad_norm": 3.998447895050049,
      "learning_rate": 8.500892325996432e-06,
      "loss": 0.9012,
      "step": 5800
    },
    {
      "epoch": 1.728138013087448,
      "grad_norm": 2.2269415855407715,
      "learning_rate": 8.481062859409083e-06,
      "loss": 0.9597,
      "step": 5810
    },
    {
      "epoch": 1.7311124330755503,
      "grad_norm": 4.0274338722229,
      "learning_rate": 8.461233392821734e-06,
      "loss": 0.8784,
      "step": 5820
    },
    {
      "epoch": 1.7340868530636526,
      "grad_norm": 1.9129562377929688,
      "learning_rate": 8.441403926234385e-06,
      "loss": 1.0832,
      "step": 5830
    },
    {
      "epoch": 1.737061273051755,
      "grad_norm": 2.873772144317627,
      "learning_rate": 8.421574459647036e-06,
      "loss": 0.9508,
      "step": 5840
    },
    {
      "epoch": 1.7400356930398573,
      "grad_norm": 2.1740567684173584,
      "learning_rate": 8.401744993059687e-06,
      "loss": 0.8899,
      "step": 5850
    },
    {
      "epoch": 1.7430101130279594,
      "grad_norm": 3.126955032348633,
      "learning_rate": 8.381915526472339e-06,
      "loss": 0.987,
      "step": 5860
    },
    {
      "epoch": 1.745984533016062,
      "grad_norm": 2.547731876373291,
      "learning_rate": 8.36208605988499e-06,
      "loss": 0.8366,
      "step": 5870
    },
    {
      "epoch": 1.748958953004164,
      "grad_norm": 2.1652748584747314,
      "learning_rate": 8.34225659329764e-06,
      "loss": 0.9147,
      "step": 5880
    },
    {
      "epoch": 1.7519333729922666,
      "grad_norm": 2.55637788772583,
      "learning_rate": 8.322427126710292e-06,
      "loss": 0.9859,
      "step": 5890
    },
    {
      "epoch": 1.7549077929803687,
      "grad_norm": 1.7500251531600952,
      "learning_rate": 8.302597660122943e-06,
      "loss": 0.9253,
      "step": 5900
    },
    {
      "epoch": 1.7578822129684712,
      "grad_norm": 1.8779126405715942,
      "learning_rate": 8.282768193535594e-06,
      "loss": 0.9757,
      "step": 5910
    },
    {
      "epoch": 1.7608566329565734,
      "grad_norm": 1.9420993328094482,
      "learning_rate": 8.262938726948245e-06,
      "loss": 0.8695,
      "step": 5920
    },
    {
      "epoch": 1.763831052944676,
      "grad_norm": 2.013251543045044,
      "learning_rate": 8.243109260360896e-06,
      "loss": 0.9584,
      "step": 5930
    },
    {
      "epoch": 1.766805472932778,
      "grad_norm": 2.415217161178589,
      "learning_rate": 8.223279793773549e-06,
      "loss": 0.8931,
      "step": 5940
    },
    {
      "epoch": 1.7697798929208806,
      "grad_norm": 2.510047435760498,
      "learning_rate": 8.2034503271862e-06,
      "loss": 0.8448,
      "step": 5950
    },
    {
      "epoch": 1.7727543129089827,
      "grad_norm": 2.8370096683502197,
      "learning_rate": 8.183620860598851e-06,
      "loss": 0.9014,
      "step": 5960
    },
    {
      "epoch": 1.7757287328970852,
      "grad_norm": 1.8006906509399414,
      "learning_rate": 8.163791394011502e-06,
      "loss": 0.8276,
      "step": 5970
    },
    {
      "epoch": 1.7787031528851873,
      "grad_norm": 2.7546730041503906,
      "learning_rate": 8.143961927424153e-06,
      "loss": 0.8385,
      "step": 5980
    },
    {
      "epoch": 1.7816775728732899,
      "grad_norm": 1.98323392868042,
      "learning_rate": 8.124132460836804e-06,
      "loss": 0.9672,
      "step": 5990
    },
    {
      "epoch": 1.784651992861392,
      "grad_norm": 2.370286464691162,
      "learning_rate": 8.104302994249456e-06,
      "loss": 1.0731,
      "step": 6000
    },
    {
      "epoch": 1.7876264128494943,
      "grad_norm": 2.672736883163452,
      "learning_rate": 8.084473527662107e-06,
      "loss": 0.9869,
      "step": 6010
    },
    {
      "epoch": 1.7906008328375966,
      "grad_norm": 2.046098232269287,
      "learning_rate": 8.064644061074758e-06,
      "loss": 0.9685,
      "step": 6020
    },
    {
      "epoch": 1.793575252825699,
      "grad_norm": 1.713912844657898,
      "learning_rate": 8.044814594487409e-06,
      "loss": 0.9631,
      "step": 6030
    },
    {
      "epoch": 1.7965496728138013,
      "grad_norm": 1.6382756233215332,
      "learning_rate": 8.02498512790006e-06,
      "loss": 0.9818,
      "step": 6040
    },
    {
      "epoch": 1.7995240928019036,
      "grad_norm": 2.065852403640747,
      "learning_rate": 8.005155661312711e-06,
      "loss": 0.9422,
      "step": 6050
    },
    {
      "epoch": 1.802498512790006,
      "grad_norm": 2.215524196624756,
      "learning_rate": 7.985326194725362e-06,
      "loss": 0.8588,
      "step": 6060
    },
    {
      "epoch": 1.8054729327781083,
      "grad_norm": 2.496154546737671,
      "learning_rate": 7.965496728138013e-06,
      "loss": 0.8197,
      "step": 6070
    },
    {
      "epoch": 1.8084473527662106,
      "grad_norm": 2.761611223220825,
      "learning_rate": 7.945667261550664e-06,
      "loss": 1.0836,
      "step": 6080
    },
    {
      "epoch": 1.811421772754313,
      "grad_norm": 2.2225403785705566,
      "learning_rate": 7.925837794963315e-06,
      "loss": 0.9185,
      "step": 6090
    },
    {
      "epoch": 1.8143961927424153,
      "grad_norm": 1.4747405052185059,
      "learning_rate": 7.906008328375967e-06,
      "loss": 0.9882,
      "step": 6100
    },
    {
      "epoch": 1.8173706127305176,
      "grad_norm": 1.7007001638412476,
      "learning_rate": 7.886178861788618e-06,
      "loss": 0.894,
      "step": 6110
    },
    {
      "epoch": 1.82034503271862,
      "grad_norm": 1.9950624704360962,
      "learning_rate": 7.866349395201269e-06,
      "loss": 0.9009,
      "step": 6120
    },
    {
      "epoch": 1.8233194527067222,
      "grad_norm": 2.18525767326355,
      "learning_rate": 7.84651992861392e-06,
      "loss": 0.9259,
      "step": 6130
    },
    {
      "epoch": 1.8262938726948246,
      "grad_norm": 2.518653392791748,
      "learning_rate": 7.826690462026573e-06,
      "loss": 0.8817,
      "step": 6140
    },
    {
      "epoch": 1.8292682926829267,
      "grad_norm": 2.0807294845581055,
      "learning_rate": 7.806860995439224e-06,
      "loss": 0.9297,
      "step": 6150
    },
    {
      "epoch": 1.8322427126710292,
      "grad_norm": 1.9742292165756226,
      "learning_rate": 7.787031528851875e-06,
      "loss": 0.9573,
      "step": 6160
    },
    {
      "epoch": 1.8352171326591313,
      "grad_norm": 2.0654001235961914,
      "learning_rate": 7.767202062264526e-06,
      "loss": 0.794,
      "step": 6170
    },
    {
      "epoch": 1.8381915526472339,
      "grad_norm": 1.837981939315796,
      "learning_rate": 7.747372595677177e-06,
      "loss": 0.9731,
      "step": 6180
    },
    {
      "epoch": 1.841165972635336,
      "grad_norm": 3.7617621421813965,
      "learning_rate": 7.727543129089828e-06,
      "loss": 0.808,
      "step": 6190
    },
    {
      "epoch": 1.8441403926234385,
      "grad_norm": 2.311782121658325,
      "learning_rate": 7.707713662502479e-06,
      "loss": 0.8762,
      "step": 6200
    },
    {
      "epoch": 1.8471148126115406,
      "grad_norm": 3.15598726272583,
      "learning_rate": 7.68788419591513e-06,
      "loss": 0.9475,
      "step": 6210
    },
    {
      "epoch": 1.8500892325996432,
      "grad_norm": 2.539973735809326,
      "learning_rate": 7.668054729327781e-06,
      "loss": 1.0267,
      "step": 6220
    },
    {
      "epoch": 1.8530636525877453,
      "grad_norm": 4.126933574676514,
      "learning_rate": 7.648225262740432e-06,
      "loss": 0.9634,
      "step": 6230
    },
    {
      "epoch": 1.8560380725758479,
      "grad_norm": 2.1178131103515625,
      "learning_rate": 7.628395796153084e-06,
      "loss": 0.7969,
      "step": 6240
    },
    {
      "epoch": 1.85901249256395,
      "grad_norm": 1.8332487344741821,
      "learning_rate": 7.6085663295657355e-06,
      "loss": 0.8832,
      "step": 6250
    },
    {
      "epoch": 1.8619869125520525,
      "grad_norm": 1.679497241973877,
      "learning_rate": 7.5887368629783865e-06,
      "loss": 0.9103,
      "step": 6260
    },
    {
      "epoch": 1.8649613325401546,
      "grad_norm": 2.227520704269409,
      "learning_rate": 7.568907396391038e-06,
      "loss": 0.8992,
      "step": 6270
    },
    {
      "epoch": 1.867935752528257,
      "grad_norm": 1.7683813571929932,
      "learning_rate": 7.549077929803689e-06,
      "loss": 0.9633,
      "step": 6280
    },
    {
      "epoch": 1.8709101725163593,
      "grad_norm": 2.328810453414917,
      "learning_rate": 7.52924846321634e-06,
      "loss": 0.9532,
      "step": 6290
    },
    {
      "epoch": 1.8738845925044616,
      "grad_norm": 2.0848748683929443,
      "learning_rate": 7.509418996628991e-06,
      "loss": 1.0087,
      "step": 6300
    },
    {
      "epoch": 1.876859012492564,
      "grad_norm": 1.935181975364685,
      "learning_rate": 7.489589530041642e-06,
      "loss": 1.0066,
      "step": 6310
    },
    {
      "epoch": 1.8798334324806663,
      "grad_norm": 3.5681967735290527,
      "learning_rate": 7.469760063454293e-06,
      "loss": 1.0778,
      "step": 6320
    },
    {
      "epoch": 1.8828078524687686,
      "grad_norm": 1.8318955898284912,
      "learning_rate": 7.449930596866944e-06,
      "loss": 1.0198,
      "step": 6330
    },
    {
      "epoch": 1.885782272456871,
      "grad_norm": 2.5921552181243896,
      "learning_rate": 7.430101130279595e-06,
      "loss": 0.9765,
      "step": 6340
    },
    {
      "epoch": 1.8887566924449732,
      "grad_norm": 2.296050786972046,
      "learning_rate": 7.410271663692248e-06,
      "loss": 0.8635,
      "step": 6350
    },
    {
      "epoch": 1.8917311124330756,
      "grad_norm": 1.662432312965393,
      "learning_rate": 7.390442197104899e-06,
      "loss": 0.898,
      "step": 6360
    },
    {
      "epoch": 1.894705532421178,
      "grad_norm": 2.867116689682007,
      "learning_rate": 7.37061273051755e-06,
      "loss": 0.9177,
      "step": 6370
    },
    {
      "epoch": 1.8976799524092802,
      "grad_norm": 2.734706401824951,
      "learning_rate": 7.350783263930201e-06,
      "loss": 0.9669,
      "step": 6380
    },
    {
      "epoch": 1.9006543723973826,
      "grad_norm": 2.152284860610962,
      "learning_rate": 7.3309537973428525e-06,
      "loss": 0.9978,
      "step": 6390
    },
    {
      "epoch": 1.9036287923854849,
      "grad_norm": 3.796322822570801,
      "learning_rate": 7.3111243307555036e-06,
      "loss": 1.0375,
      "step": 6400
    },
    {
      "epoch": 1.9066032123735872,
      "grad_norm": 2.5495288372039795,
      "learning_rate": 7.291294864168155e-06,
      "loss": 0.9448,
      "step": 6410
    },
    {
      "epoch": 1.9095776323616893,
      "grad_norm": 2.3498852252960205,
      "learning_rate": 7.271465397580806e-06,
      "loss": 0.9316,
      "step": 6420
    },
    {
      "epoch": 1.9125520523497919,
      "grad_norm": 2.3854613304138184,
      "learning_rate": 7.251635930993457e-06,
      "loss": 0.9013,
      "step": 6430
    },
    {
      "epoch": 1.915526472337894,
      "grad_norm": 1.8311249017715454,
      "learning_rate": 7.231806464406108e-06,
      "loss": 0.8911,
      "step": 6440
    },
    {
      "epoch": 1.9185008923259965,
      "grad_norm": 2.2831578254699707,
      "learning_rate": 7.211976997818759e-06,
      "loss": 0.8981,
      "step": 6450
    },
    {
      "epoch": 1.9214753123140986,
      "grad_norm": 1.8435122966766357,
      "learning_rate": 7.19214753123141e-06,
      "loss": 0.9857,
      "step": 6460
    },
    {
      "epoch": 1.9244497323022012,
      "grad_norm": 2.2343404293060303,
      "learning_rate": 7.172318064644061e-06,
      "loss": 0.9633,
      "step": 6470
    },
    {
      "epoch": 1.9274241522903033,
      "grad_norm": 3.6595189571380615,
      "learning_rate": 7.152488598056712e-06,
      "loss": 1.0294,
      "step": 6480
    },
    {
      "epoch": 1.9303985722784058,
      "grad_norm": 1.9619334936141968,
      "learning_rate": 7.132659131469364e-06,
      "loss": 0.9405,
      "step": 6490
    },
    {
      "epoch": 1.933372992266508,
      "grad_norm": 2.6572554111480713,
      "learning_rate": 7.112829664882015e-06,
      "loss": 1.0347,
      "step": 6500
    },
    {
      "epoch": 1.9363474122546105,
      "grad_norm": 1.8363322019577026,
      "learning_rate": 7.0930001982946664e-06,
      "loss": 0.9482,
      "step": 6510
    },
    {
      "epoch": 1.9393218322427126,
      "grad_norm": 1.788817286491394,
      "learning_rate": 7.0731707317073175e-06,
      "loss": 0.9005,
      "step": 6520
    },
    {
      "epoch": 1.9422962522308151,
      "grad_norm": 2.169330358505249,
      "learning_rate": 7.053341265119969e-06,
      "loss": 1.0485,
      "step": 6530
    },
    {
      "epoch": 1.9452706722189173,
      "grad_norm": 2.868863821029663,
      "learning_rate": 7.03351179853262e-06,
      "loss": 0.8012,
      "step": 6540
    },
    {
      "epoch": 1.9482450922070196,
      "grad_norm": 3.19114351272583,
      "learning_rate": 7.013682331945271e-06,
      "loss": 0.8314,
      "step": 6550
    },
    {
      "epoch": 1.951219512195122,
      "grad_norm": 1.8040211200714111,
      "learning_rate": 6.993852865357923e-06,
      "loss": 0.9234,
      "step": 6560
    },
    {
      "epoch": 1.9541939321832242,
      "grad_norm": 1.4258449077606201,
      "learning_rate": 6.974023398770574e-06,
      "loss": 0.9112,
      "step": 6570
    },
    {
      "epoch": 1.9571683521713266,
      "grad_norm": 2.0457892417907715,
      "learning_rate": 6.954193932183225e-06,
      "loss": 0.9615,
      "step": 6580
    },
    {
      "epoch": 1.960142772159429,
      "grad_norm": 2.13922381401062,
      "learning_rate": 6.934364465595876e-06,
      "loss": 0.9923,
      "step": 6590
    },
    {
      "epoch": 1.9631171921475312,
      "grad_norm": 1.9266884326934814,
      "learning_rate": 6.914534999008527e-06,
      "loss": 0.8837,
      "step": 6600
    },
    {
      "epoch": 1.9660916121356335,
      "grad_norm": 2.1289117336273193,
      "learning_rate": 6.894705532421179e-06,
      "loss": 0.9102,
      "step": 6610
    },
    {
      "epoch": 1.9690660321237359,
      "grad_norm": 3.1423187255859375,
      "learning_rate": 6.87487606583383e-06,
      "loss": 1.0147,
      "step": 6620
    },
    {
      "epoch": 1.9720404521118382,
      "grad_norm": 3.466738224029541,
      "learning_rate": 6.855046599246481e-06,
      "loss": 1.062,
      "step": 6630
    },
    {
      "epoch": 1.9750148720999405,
      "grad_norm": 4.174920082092285,
      "learning_rate": 6.835217132659132e-06,
      "loss": 0.9672,
      "step": 6640
    },
    {
      "epoch": 1.9779892920880429,
      "grad_norm": 5.713836193084717,
      "learning_rate": 6.8153876660717834e-06,
      "loss": 0.8793,
      "step": 6650
    },
    {
      "epoch": 1.9809637120761452,
      "grad_norm": 2.2384696006774902,
      "learning_rate": 6.7955581994844345e-06,
      "loss": 0.9204,
      "step": 6660
    },
    {
      "epoch": 1.9839381320642475,
      "grad_norm": 2.094909191131592,
      "learning_rate": 6.775728732897086e-06,
      "loss": 0.9848,
      "step": 6670
    },
    {
      "epoch": 1.9869125520523498,
      "grad_norm": 1.672589898109436,
      "learning_rate": 6.755899266309737e-06,
      "loss": 0.7789,
      "step": 6680
    },
    {
      "epoch": 1.989886972040452,
      "grad_norm": 2.561947822570801,
      "learning_rate": 6.736069799722388e-06,
      "loss": 0.9263,
      "step": 6690
    },
    {
      "epoch": 1.9928613920285545,
      "grad_norm": 2.6096744537353516,
      "learning_rate": 6.716240333135039e-06,
      "loss": 0.9434,
      "step": 6700
    },
    {
      "epoch": 1.9958358120166566,
      "grad_norm": 2.392299175262451,
      "learning_rate": 6.69641086654769e-06,
      "loss": 0.9836,
      "step": 6710
    },
    {
      "epoch": 1.9988102320047592,
      "grad_norm": 1.8176876306533813,
      "learning_rate": 6.676581399960341e-06,
      "loss": 0.8861,
      "step": 6720
    },
    {
      "epoch": 2.0,
      "eval_bleu": 0.32468159990304507,
      "eval_loss": 0.8126429319381714,
      "eval_runtime": 165.3822,
      "eval_samples_per_second": 9.034,
      "eval_steps_per_second": 2.261,
      "step": 6724
    },
    {
      "epoch": 2.0017846519928613,
      "grad_norm": 1.6424808502197266,
      "learning_rate": 6.656751933372992e-06,
      "loss": 0.8039,
      "step": 6730
    },
    {
      "epoch": 2.004759071980964,
      "grad_norm": 1.5823683738708496,
      "learning_rate": 6.636922466785643e-06,
      "loss": 0.7681,
      "step": 6740
    },
    {
      "epoch": 2.007733491969066,
      "grad_norm": 2.1572327613830566,
      "learning_rate": 6.617093000198294e-06,
      "loss": 0.9987,
      "step": 6750
    },
    {
      "epoch": 2.0107079119571685,
      "grad_norm": 2.767049551010132,
      "learning_rate": 6.597263533610947e-06,
      "loss": 0.9606,
      "step": 6760
    },
    {
      "epoch": 2.0136823319452706,
      "grad_norm": 1.7000491619110107,
      "learning_rate": 6.577434067023598e-06,
      "loss": 0.9872,
      "step": 6770
    },
    {
      "epoch": 2.016656751933373,
      "grad_norm": 2.1042919158935547,
      "learning_rate": 6.557604600436249e-06,
      "loss": 0.9875,
      "step": 6780
    },
    {
      "epoch": 2.0196311719214752,
      "grad_norm": 2.009009599685669,
      "learning_rate": 6.5377751338489005e-06,
      "loss": 0.8333,
      "step": 6790
    },
    {
      "epoch": 2.022605591909578,
      "grad_norm": 2.1507396697998047,
      "learning_rate": 6.5179456672615515e-06,
      "loss": 1.0091,
      "step": 6800
    },
    {
      "epoch": 2.02558001189768,
      "grad_norm": 2.4801650047302246,
      "learning_rate": 6.498116200674203e-06,
      "loss": 1.0486,
      "step": 6810
    },
    {
      "epoch": 2.0285544318857824,
      "grad_norm": 1.8804681301116943,
      "learning_rate": 6.478286734086854e-06,
      "loss": 0.9156,
      "step": 6820
    },
    {
      "epoch": 2.0315288518738845,
      "grad_norm": 1.9919235706329346,
      "learning_rate": 6.458457267499505e-06,
      "loss": 0.9184,
      "step": 6830
    },
    {
      "epoch": 2.034503271861987,
      "grad_norm": 2.438093900680542,
      "learning_rate": 6.438627800912156e-06,
      "loss": 0.9138,
      "step": 6840
    },
    {
      "epoch": 2.037477691850089,
      "grad_norm": 1.7430202960968018,
      "learning_rate": 6.418798334324807e-06,
      "loss": 0.943,
      "step": 6850
    },
    {
      "epoch": 2.0404521118381918,
      "grad_norm": 1.831638216972351,
      "learning_rate": 6.398968867737458e-06,
      "loss": 0.848,
      "step": 6860
    },
    {
      "epoch": 2.043426531826294,
      "grad_norm": 2.113398790359497,
      "learning_rate": 6.379139401150109e-06,
      "loss": 0.9095,
      "step": 6870
    },
    {
      "epoch": 2.0464009518143964,
      "grad_norm": 7.6439080238342285,
      "learning_rate": 6.35930993456276e-06,
      "loss": 0.9503,
      "step": 6880
    },
    {
      "epoch": 2.0493753718024985,
      "grad_norm": 2.714089870452881,
      "learning_rate": 6.339480467975412e-06,
      "loss": 0.8905,
      "step": 6890
    },
    {
      "epoch": 2.0523497917906006,
      "grad_norm": 1.6275670528411865,
      "learning_rate": 6.319651001388063e-06,
      "loss": 0.9895,
      "step": 6900
    },
    {
      "epoch": 2.055324211778703,
      "grad_norm": 2.3396949768066406,
      "learning_rate": 6.2998215348007144e-06,
      "loss": 1.0027,
      "step": 6910
    },
    {
      "epoch": 2.0582986317668053,
      "grad_norm": 2.499920606613159,
      "learning_rate": 6.2799920682133655e-06,
      "loss": 0.9112,
      "step": 6920
    },
    {
      "epoch": 2.061273051754908,
      "grad_norm": 4.920656681060791,
      "learning_rate": 6.260162601626017e-06,
      "loss": 0.8875,
      "step": 6930
    },
    {
      "epoch": 2.06424747174301,
      "grad_norm": 2.1055476665496826,
      "learning_rate": 6.240333135038668e-06,
      "loss": 0.8246,
      "step": 6940
    },
    {
      "epoch": 2.0672218917311125,
      "grad_norm": 1.8014905452728271,
      "learning_rate": 6.220503668451319e-06,
      "loss": 0.8268,
      "step": 6950
    },
    {
      "epoch": 2.0701963117192146,
      "grad_norm": 2.1915528774261475,
      "learning_rate": 6.20067420186397e-06,
      "loss": 0.8269,
      "step": 6960
    },
    {
      "epoch": 2.073170731707317,
      "grad_norm": 1.8992480039596558,
      "learning_rate": 6.180844735276622e-06,
      "loss": 1.0096,
      "step": 6970
    },
    {
      "epoch": 2.0761451516954192,
      "grad_norm": 2.3679513931274414,
      "learning_rate": 6.161015268689273e-06,
      "loss": 0.9843,
      "step": 6980
    },
    {
      "epoch": 2.079119571683522,
      "grad_norm": 1.9892520904541016,
      "learning_rate": 6.141185802101924e-06,
      "loss": 1.0426,
      "step": 6990
    },
    {
      "epoch": 2.082093991671624,
      "grad_norm": 3.660391092300415,
      "learning_rate": 6.121356335514575e-06,
      "loss": 0.9421,
      "step": 7000
    },
    {
      "epoch": 2.0850684116597265,
      "grad_norm": 2.070767641067505,
      "learning_rate": 6.101526868927227e-06,
      "loss": 0.8968,
      "step": 7010
    },
    {
      "epoch": 2.0880428316478286,
      "grad_norm": 2.106721878051758,
      "learning_rate": 6.081697402339878e-06,
      "loss": 0.9492,
      "step": 7020
    },
    {
      "epoch": 2.091017251635931,
      "grad_norm": 2.6034016609191895,
      "learning_rate": 6.061867935752529e-06,
      "loss": 0.8203,
      "step": 7030
    },
    {
      "epoch": 2.093991671624033,
      "grad_norm": 8.420632362365723,
      "learning_rate": 6.04203846916518e-06,
      "loss": 1.008,
      "step": 7040
    },
    {
      "epoch": 2.0969660916121358,
      "grad_norm": 1.9408552646636963,
      "learning_rate": 6.0222090025778314e-06,
      "loss": 0.8463,
      "step": 7050
    },
    {
      "epoch": 2.099940511600238,
      "grad_norm": 2.5865306854248047,
      "learning_rate": 6.0023795359904825e-06,
      "loss": 1.0568,
      "step": 7060
    },
    {
      "epoch": 2.1029149315883404,
      "grad_norm": 1.4122792482376099,
      "learning_rate": 5.982550069403134e-06,
      "loss": 0.8962,
      "step": 7070
    },
    {
      "epoch": 2.1058893515764425,
      "grad_norm": 1.8262698650360107,
      "learning_rate": 5.962720602815785e-06,
      "loss": 0.9119,
      "step": 7080
    },
    {
      "epoch": 2.108863771564545,
      "grad_norm": 4.503444671630859,
      "learning_rate": 5.942891136228436e-06,
      "loss": 0.9531,
      "step": 7090
    },
    {
      "epoch": 2.111838191552647,
      "grad_norm": 2.1308577060699463,
      "learning_rate": 5.923061669641087e-06,
      "loss": 0.9085,
      "step": 7100
    },
    {
      "epoch": 2.1148126115407497,
      "grad_norm": 2.348580837249756,
      "learning_rate": 5.903232203053738e-06,
      "loss": 0.828,
      "step": 7110
    },
    {
      "epoch": 2.117787031528852,
      "grad_norm": 2.0505149364471436,
      "learning_rate": 5.883402736466389e-06,
      "loss": 0.9747,
      "step": 7120
    },
    {
      "epoch": 2.1207614515169544,
      "grad_norm": 5.167563438415527,
      "learning_rate": 5.86357326987904e-06,
      "loss": 0.9467,
      "step": 7130
    },
    {
      "epoch": 2.1237358715050565,
      "grad_norm": 2.362130641937256,
      "learning_rate": 5.843743803291691e-06,
      "loss": 0.8659,
      "step": 7140
    },
    {
      "epoch": 2.1267102914931586,
      "grad_norm": 2.4962692260742188,
      "learning_rate": 5.823914336704342e-06,
      "loss": 0.9607,
      "step": 7150
    },
    {
      "epoch": 2.129684711481261,
      "grad_norm": 1.9215614795684814,
      "learning_rate": 5.8040848701169935e-06,
      "loss": 0.8948,
      "step": 7160
    },
    {
      "epoch": 2.1326591314693637,
      "grad_norm": 2.1289968490600586,
      "learning_rate": 5.784255403529645e-06,
      "loss": 0.9193,
      "step": 7170
    },
    {
      "epoch": 2.135633551457466,
      "grad_norm": 1.9786211252212524,
      "learning_rate": 5.764425936942297e-06,
      "loss": 0.8305,
      "step": 7180
    },
    {
      "epoch": 2.138607971445568,
      "grad_norm": 1.8785942792892456,
      "learning_rate": 5.7445964703549484e-06,
      "loss": 0.9325,
      "step": 7190
    },
    {
      "epoch": 2.1415823914336705,
      "grad_norm": 2.622451066970825,
      "learning_rate": 5.7247670037675995e-06,
      "loss": 0.8639,
      "step": 7200
    },
    {
      "epoch": 2.1445568114217726,
      "grad_norm": 2.259632110595703,
      "learning_rate": 5.704937537180251e-06,
      "loss": 0.9268,
      "step": 7210
    },
    {
      "epoch": 2.147531231409875,
      "grad_norm": 2.519744873046875,
      "learning_rate": 5.685108070592902e-06,
      "loss": 0.7806,
      "step": 7220
    },
    {
      "epoch": 2.1505056513979772,
      "grad_norm": 2.258423089981079,
      "learning_rate": 5.665278604005553e-06,
      "loss": 0.7901,
      "step": 7230
    },
    {
      "epoch": 2.1534800713860798,
      "grad_norm": 1.936894178390503,
      "learning_rate": 5.645449137418204e-06,
      "loss": 0.9245,
      "step": 7240
    },
    {
      "epoch": 2.156454491374182,
      "grad_norm": 2.566894292831421,
      "learning_rate": 5.625619670830855e-06,
      "loss": 0.9792,
      "step": 7250
    },
    {
      "epoch": 2.1594289113622844,
      "grad_norm": 2.9060206413269043,
      "learning_rate": 5.605790204243506e-06,
      "loss": 0.9881,
      "step": 7260
    },
    {
      "epoch": 2.1624033313503865,
      "grad_norm": 1.995665192604065,
      "learning_rate": 5.585960737656157e-06,
      "loss": 0.944,
      "step": 7270
    },
    {
      "epoch": 2.165377751338489,
      "grad_norm": 6.062785625457764,
      "learning_rate": 5.566131271068808e-06,
      "loss": 0.9474,
      "step": 7280
    },
    {
      "epoch": 2.168352171326591,
      "grad_norm": 4.651455402374268,
      "learning_rate": 5.54630180448146e-06,
      "loss": 0.9582,
      "step": 7290
    },
    {
      "epoch": 2.1713265913146937,
      "grad_norm": 1.9725854396820068,
      "learning_rate": 5.526472337894111e-06,
      "loss": 0.8431,
      "step": 7300
    },
    {
      "epoch": 2.174301011302796,
      "grad_norm": 1.801163673400879,
      "learning_rate": 5.506642871306762e-06,
      "loss": 0.8217,
      "step": 7310
    },
    {
      "epoch": 2.1772754312908984,
      "grad_norm": 2.521217107772827,
      "learning_rate": 5.4868134047194135e-06,
      "loss": 0.8531,
      "step": 7320
    },
    {
      "epoch": 2.1802498512790005,
      "grad_norm": 2.052593469619751,
      "learning_rate": 5.466983938132065e-06,
      "loss": 0.7693,
      "step": 7330
    },
    {
      "epoch": 2.183224271267103,
      "grad_norm": 1.604993462562561,
      "learning_rate": 5.447154471544716e-06,
      "loss": 0.8151,
      "step": 7340
    },
    {
      "epoch": 2.186198691255205,
      "grad_norm": 1.9920135736465454,
      "learning_rate": 5.427325004957367e-06,
      "loss": 1.004,
      "step": 7350
    },
    {
      "epoch": 2.1891731112433077,
      "grad_norm": 2.493720054626465,
      "learning_rate": 5.407495538370018e-06,
      "loss": 0.8186,
      "step": 7360
    },
    {
      "epoch": 2.19214753123141,
      "grad_norm": 4.132340908050537,
      "learning_rate": 5.387666071782669e-06,
      "loss": 0.8963,
      "step": 7370
    },
    {
      "epoch": 2.1951219512195124,
      "grad_norm": 2.5352697372436523,
      "learning_rate": 5.367836605195321e-06,
      "loss": 1.0509,
      "step": 7380
    },
    {
      "epoch": 2.1980963712076145,
      "grad_norm": 1.6684036254882812,
      "learning_rate": 5.348007138607972e-06,
      "loss": 0.7771,
      "step": 7390
    },
    {
      "epoch": 2.201070791195717,
      "grad_norm": 1.8032926321029663,
      "learning_rate": 5.328177672020623e-06,
      "loss": 0.8436,
      "step": 7400
    },
    {
      "epoch": 2.204045211183819,
      "grad_norm": 2.8459277153015137,
      "learning_rate": 5.308348205433275e-06,
      "loss": 0.9032,
      "step": 7410
    },
    {
      "epoch": 2.2070196311719217,
      "grad_norm": 2.9243156909942627,
      "learning_rate": 5.288518738845926e-06,
      "loss": 0.8831,
      "step": 7420
    },
    {
      "epoch": 2.209994051160024,
      "grad_norm": 2.0995869636535645,
      "learning_rate": 5.268689272258577e-06,
      "loss": 0.8087,
      "step": 7430
    },
    {
      "epoch": 2.212968471148126,
      "grad_norm": 1.6311087608337402,
      "learning_rate": 5.248859805671228e-06,
      "loss": 0.848,
      "step": 7440
    },
    {
      "epoch": 2.2159428911362284,
      "grad_norm": 2.4918603897094727,
      "learning_rate": 5.2290303390838794e-06,
      "loss": 0.8669,
      "step": 7450
    },
    {
      "epoch": 2.2189173111243305,
      "grad_norm": 2.6099472045898438,
      "learning_rate": 5.2092008724965305e-06,
      "loss": 0.8332,
      "step": 7460
    },
    {
      "epoch": 2.221891731112433,
      "grad_norm": 1.9156285524368286,
      "learning_rate": 5.189371405909182e-06,
      "loss": 0.9424,
      "step": 7470
    },
    {
      "epoch": 2.224866151100535,
      "grad_norm": 2.2306435108184814,
      "learning_rate": 5.169541939321833e-06,
      "loss": 0.8696,
      "step": 7480
    },
    {
      "epoch": 2.2278405710886378,
      "grad_norm": 2.0106942653656006,
      "learning_rate": 5.149712472734484e-06,
      "loss": 0.9785,
      "step": 7490
    },
    {
      "epoch": 2.23081499107674,
      "grad_norm": 3.4017693996429443,
      "learning_rate": 5.129883006147135e-06,
      "loss": 0.8255,
      "step": 7500
    },
    {
      "epoch": 2.2337894110648424,
      "grad_norm": 1.9775711297988892,
      "learning_rate": 5.110053539559786e-06,
      "loss": 1.0719,
      "step": 7510
    },
    {
      "epoch": 2.2367638310529445,
      "grad_norm": 2.1127076148986816,
      "learning_rate": 5.090224072972437e-06,
      "loss": 0.8873,
      "step": 7520
    },
    {
      "epoch": 2.239738251041047,
      "grad_norm": 22.273656845092773,
      "learning_rate": 5.070394606385088e-06,
      "loss": 0.8655,
      "step": 7530
    },
    {
      "epoch": 2.242712671029149,
      "grad_norm": 2.1085987091064453,
      "learning_rate": 5.050565139797739e-06,
      "loss": 0.9387,
      "step": 7540
    },
    {
      "epoch": 2.2456870910172517,
      "grad_norm": 2.261594295501709,
      "learning_rate": 5.03073567321039e-06,
      "loss": 0.8232,
      "step": 7550
    },
    {
      "epoch": 2.248661511005354,
      "grad_norm": 2.2048513889312744,
      "learning_rate": 5.0109062066230415e-06,
      "loss": 0.9024,
      "step": 7560
    },
    {
      "epoch": 2.2516359309934564,
      "grad_norm": 1.8168585300445557,
      "learning_rate": 4.991076740035693e-06,
      "loss": 0.9379,
      "step": 7570
    },
    {
      "epoch": 2.2546103509815585,
      "grad_norm": 1.5834389925003052,
      "learning_rate": 4.9712472734483445e-06,
      "loss": 0.8947,
      "step": 7580
    },
    {
      "epoch": 2.257584770969661,
      "grad_norm": 1.6733750104904175,
      "learning_rate": 4.951417806860996e-06,
      "loss": 0.8248,
      "step": 7590
    },
    {
      "epoch": 2.260559190957763,
      "grad_norm": 2.07000470161438,
      "learning_rate": 4.931588340273647e-06,
      "loss": 1.0339,
      "step": 7600
    },
    {
      "epoch": 2.2635336109458657,
      "grad_norm": 2.0652945041656494,
      "learning_rate": 4.911758873686298e-06,
      "loss": 0.9709,
      "step": 7610
    },
    {
      "epoch": 2.266508030933968,
      "grad_norm": 1.9922059774398804,
      "learning_rate": 4.891929407098949e-06,
      "loss": 0.8785,
      "step": 7620
    },
    {
      "epoch": 2.2694824509220703,
      "grad_norm": 1.8817555904388428,
      "learning_rate": 4.872099940511601e-06,
      "loss": 0.8254,
      "step": 7630
    },
    {
      "epoch": 2.2724568709101725,
      "grad_norm": 1.8859727382659912,
      "learning_rate": 4.852270473924252e-06,
      "loss": 0.9277,
      "step": 7640
    },
    {
      "epoch": 2.275431290898275,
      "grad_norm": 1.9276251792907715,
      "learning_rate": 4.832441007336903e-06,
      "loss": 0.8449,
      "step": 7650
    },
    {
      "epoch": 2.278405710886377,
      "grad_norm": 9.826400756835938,
      "learning_rate": 4.812611540749554e-06,
      "loss": 0.916,
      "step": 7660
    },
    {
      "epoch": 2.2813801308744797,
      "grad_norm": 2.364476203918457,
      "learning_rate": 4.792782074162205e-06,
      "loss": 0.9748,
      "step": 7670
    },
    {
      "epoch": 2.2843545508625818,
      "grad_norm": 3.2580878734588623,
      "learning_rate": 4.772952607574856e-06,
      "loss": 1.0338,
      "step": 7680
    },
    {
      "epoch": 2.287328970850684,
      "grad_norm": 2.084820032119751,
      "learning_rate": 4.753123140987508e-06,
      "loss": 0.9866,
      "step": 7690
    },
    {
      "epoch": 2.2903033908387864,
      "grad_norm": 3.849170207977295,
      "learning_rate": 4.733293674400159e-06,
      "loss": 0.8348,
      "step": 7700
    },
    {
      "epoch": 2.293277810826889,
      "grad_norm": 1.6001389026641846,
      "learning_rate": 4.71346420781281e-06,
      "loss": 0.879,
      "step": 7710
    },
    {
      "epoch": 2.296252230814991,
      "grad_norm": 2.156172275543213,
      "learning_rate": 4.6936347412254615e-06,
      "loss": 0.9111,
      "step": 7720
    },
    {
      "epoch": 2.299226650803093,
      "grad_norm": 1.8805220127105713,
      "learning_rate": 4.673805274638113e-06,
      "loss": 0.9822,
      "step": 7730
    },
    {
      "epoch": 2.3022010707911957,
      "grad_norm": 2.3899548053741455,
      "learning_rate": 4.653975808050764e-06,
      "loss": 0.837,
      "step": 7740
    },
    {
      "epoch": 2.3051754907792983,
      "grad_norm": 1.8951066732406616,
      "learning_rate": 4.634146341463416e-06,
      "loss": 0.9671,
      "step": 7750
    },
    {
      "epoch": 2.3081499107674004,
      "grad_norm": 1.77108895778656,
      "learning_rate": 4.614316874876067e-06,
      "loss": 0.8708,
      "step": 7760
    },
    {
      "epoch": 2.3111243307555025,
      "grad_norm": 2.426016092300415,
      "learning_rate": 4.594487408288718e-06,
      "loss": 1.0607,
      "step": 7770
    },
    {
      "epoch": 2.314098750743605,
      "grad_norm": 2.280961275100708,
      "learning_rate": 4.574657941701369e-06,
      "loss": 0.8949,
      "step": 7780
    },
    {
      "epoch": 2.317073170731707,
      "grad_norm": 1.9045838117599487,
      "learning_rate": 4.55482847511402e-06,
      "loss": 0.8101,
      "step": 7790
    },
    {
      "epoch": 2.3200475907198097,
      "grad_norm": 2.7751195430755615,
      "learning_rate": 4.534999008526671e-06,
      "loss": 0.9303,
      "step": 7800
    },
    {
      "epoch": 2.323022010707912,
      "grad_norm": 1.9886611700057983,
      "learning_rate": 4.515169541939322e-06,
      "loss": 0.8503,
      "step": 7810
    },
    {
      "epoch": 2.3259964306960144,
      "grad_norm": 1.8103457689285278,
      "learning_rate": 4.495340075351973e-06,
      "loss": 0.9253,
      "step": 7820
    },
    {
      "epoch": 2.3289708506841165,
      "grad_norm": 2.824070930480957,
      "learning_rate": 4.475510608764624e-06,
      "loss": 0.7627,
      "step": 7830
    },
    {
      "epoch": 2.331945270672219,
      "grad_norm": 1.8347276449203491,
      "learning_rate": 4.4556811421772755e-06,
      "loss": 0.8967,
      "step": 7840
    },
    {
      "epoch": 2.334919690660321,
      "grad_norm": 2.0562851428985596,
      "learning_rate": 4.435851675589927e-06,
      "loss": 0.8995,
      "step": 7850
    },
    {
      "epoch": 2.3378941106484237,
      "grad_norm": 3.3362948894500732,
      "learning_rate": 4.4160222090025785e-06,
      "loss": 0.9323,
      "step": 7860
    },
    {
      "epoch": 2.340868530636526,
      "grad_norm": 1.7112445831298828,
      "learning_rate": 4.39619274241523e-06,
      "loss": 0.9516,
      "step": 7870
    },
    {
      "epoch": 2.3438429506246283,
      "grad_norm": 1.8435856103897095,
      "learning_rate": 4.376363275827881e-06,
      "loss": 0.9206,
      "step": 7880
    },
    {
      "epoch": 2.3468173706127304,
      "grad_norm": 2.1342930793762207,
      "learning_rate": 4.356533809240532e-06,
      "loss": 0.9373,
      "step": 7890
    },
    {
      "epoch": 2.349791790600833,
      "grad_norm": 2.836467981338501,
      "learning_rate": 4.336704342653183e-06,
      "loss": 0.9392,
      "step": 7900
    },
    {
      "epoch": 2.352766210588935,
      "grad_norm": 1.8344311714172363,
      "learning_rate": 4.316874876065834e-06,
      "loss": 0.94,
      "step": 7910
    },
    {
      "epoch": 2.3557406305770376,
      "grad_norm": 2.3108370304107666,
      "learning_rate": 4.297045409478485e-06,
      "loss": 0.883,
      "step": 7920
    },
    {
      "epoch": 2.3587150505651397,
      "grad_norm": 5.176657676696777,
      "learning_rate": 4.277215942891136e-06,
      "loss": 0.8553,
      "step": 7930
    },
    {
      "epoch": 2.3616894705532423,
      "grad_norm": 2.015786647796631,
      "learning_rate": 4.257386476303787e-06,
      "loss": 0.9348,
      "step": 7940
    },
    {
      "epoch": 2.3646638905413444,
      "grad_norm": 2.9428205490112305,
      "learning_rate": 4.237557009716439e-06,
      "loss": 0.8885,
      "step": 7950
    },
    {
      "epoch": 2.367638310529447,
      "grad_norm": 2.195396900177002,
      "learning_rate": 4.21772754312909e-06,
      "loss": 0.9405,
      "step": 7960
    },
    {
      "epoch": 2.370612730517549,
      "grad_norm": 3.4856903553009033,
      "learning_rate": 4.197898076541741e-06,
      "loss": 0.9077,
      "step": 7970
    },
    {
      "epoch": 2.373587150505651,
      "grad_norm": 2.03555965423584,
      "learning_rate": 4.1780686099543925e-06,
      "loss": 0.9196,
      "step": 7980
    },
    {
      "epoch": 2.3765615704937537,
      "grad_norm": 2.751368761062622,
      "learning_rate": 4.158239143367044e-06,
      "loss": 0.9281,
      "step": 7990
    },
    {
      "epoch": 2.3795359904818563,
      "grad_norm": 2.566455602645874,
      "learning_rate": 4.138409676779695e-06,
      "loss": 0.9207,
      "step": 8000
    },
    {
      "epoch": 2.3825104104699584,
      "grad_norm": 1.5994690656661987,
      "learning_rate": 4.118580210192346e-06,
      "loss": 0.9118,
      "step": 8010
    },
    {
      "epoch": 2.3854848304580605,
      "grad_norm": 2.15860915184021,
      "learning_rate": 4.098750743604997e-06,
      "loss": 0.8785,
      "step": 8020
    },
    {
      "epoch": 2.388459250446163,
      "grad_norm": 2.486734390258789,
      "learning_rate": 4.078921277017649e-06,
      "loss": 0.8552,
      "step": 8030
    },
    {
      "epoch": 2.391433670434265,
      "grad_norm": 1.658303141593933,
      "learning_rate": 4.0590918104303e-06,
      "loss": 0.9165,
      "step": 8040
    },
    {
      "epoch": 2.3944080904223677,
      "grad_norm": 2.7804839611053467,
      "learning_rate": 4.039262343842951e-06,
      "loss": 0.9021,
      "step": 8050
    },
    {
      "epoch": 2.39738251041047,
      "grad_norm": 2.4038166999816895,
      "learning_rate": 4.019432877255602e-06,
      "loss": 0.918,
      "step": 8060
    },
    {
      "epoch": 2.4003569303985723,
      "grad_norm": 2.1479227542877197,
      "learning_rate": 3.999603410668253e-06,
      "loss": 0.8977,
      "step": 8070
    },
    {
      "epoch": 2.4033313503866744,
      "grad_norm": 2.608168840408325,
      "learning_rate": 3.979773944080904e-06,
      "loss": 0.8008,
      "step": 8080
    },
    {
      "epoch": 2.406305770374777,
      "grad_norm": 2.2984840869903564,
      "learning_rate": 3.959944477493556e-06,
      "loss": 0.9456,
      "step": 8090
    },
    {
      "epoch": 2.409280190362879,
      "grad_norm": 1.9903775453567505,
      "learning_rate": 3.940115010906207e-06,
      "loss": 0.8965,
      "step": 8100
    },
    {
      "epoch": 2.4122546103509817,
      "grad_norm": 1.9953843355178833,
      "learning_rate": 3.920285544318858e-06,
      "loss": 0.8795,
      "step": 8110
    },
    {
      "epoch": 2.4152290303390838,
      "grad_norm": 1.6764146089553833,
      "learning_rate": 3.9004560777315095e-06,
      "loss": 0.8353,
      "step": 8120
    },
    {
      "epoch": 2.4182034503271863,
      "grad_norm": 3.0514702796936035,
      "learning_rate": 3.880626611144161e-06,
      "loss": 0.9994,
      "step": 8130
    },
    {
      "epoch": 2.4211778703152884,
      "grad_norm": 1.9703104496002197,
      "learning_rate": 3.860797144556812e-06,
      "loss": 0.9262,
      "step": 8140
    },
    {
      "epoch": 2.424152290303391,
      "grad_norm": 1.5955092906951904,
      "learning_rate": 3.840967677969463e-06,
      "loss": 0.8699,
      "step": 8150
    },
    {
      "epoch": 2.427126710291493,
      "grad_norm": 2.4208266735076904,
      "learning_rate": 3.821138211382115e-06,
      "loss": 0.8958,
      "step": 8160
    },
    {
      "epoch": 2.4301011302795956,
      "grad_norm": 2.89404034614563,
      "learning_rate": 3.8013087447947654e-06,
      "loss": 0.9701,
      "step": 8170
    },
    {
      "epoch": 2.4330755502676977,
      "grad_norm": 1.6904550790786743,
      "learning_rate": 3.7814792782074165e-06,
      "loss": 0.769,
      "step": 8180
    },
    {
      "epoch": 2.4360499702558003,
      "grad_norm": 2.089080333709717,
      "learning_rate": 3.761649811620068e-06,
      "loss": 0.8042,
      "step": 8190
    },
    {
      "epoch": 2.4390243902439024,
      "grad_norm": 1.801967978477478,
      "learning_rate": 3.741820345032719e-06,
      "loss": 0.9542,
      "step": 8200
    },
    {
      "epoch": 2.441998810232005,
      "grad_norm": 3.888838291168213,
      "learning_rate": 3.72199087844537e-06,
      "loss": 0.8602,
      "step": 8210
    },
    {
      "epoch": 2.444973230220107,
      "grad_norm": 2.2247586250305176,
      "learning_rate": 3.7021614118580213e-06,
      "loss": 0.9369,
      "step": 8220
    },
    {
      "epoch": 2.4479476502082096,
      "grad_norm": 2.0758142471313477,
      "learning_rate": 3.6823319452706724e-06,
      "loss": 0.8987,
      "step": 8230
    },
    {
      "epoch": 2.4509220701963117,
      "grad_norm": 1.8356307744979858,
      "learning_rate": 3.6625024786833235e-06,
      "loss": 0.8125,
      "step": 8240
    },
    {
      "epoch": 2.4538964901844142,
      "grad_norm": 1.8998695611953735,
      "learning_rate": 3.6426730120959746e-06,
      "loss": 1.0167,
      "step": 8250
    },
    {
      "epoch": 2.4568709101725164,
      "grad_norm": 2.2008297443389893,
      "learning_rate": 3.6228435455086265e-06,
      "loss": 0.836,
      "step": 8260
    },
    {
      "epoch": 2.4598453301606185,
      "grad_norm": 2.5543503761291504,
      "learning_rate": 3.6030140789212776e-06,
      "loss": 0.928,
      "step": 8270
    },
    {
      "epoch": 2.462819750148721,
      "grad_norm": 1.4702887535095215,
      "learning_rate": 3.5831846123339287e-06,
      "loss": 1.0075,
      "step": 8280
    },
    {
      "epoch": 2.4657941701368236,
      "grad_norm": 2.97819447517395,
      "learning_rate": 3.5633551457465798e-06,
      "loss": 0.8679,
      "step": 8290
    },
    {
      "epoch": 2.4687685901249257,
      "grad_norm": 1.6785544157028198,
      "learning_rate": 3.543525679159231e-06,
      "loss": 0.8181,
      "step": 8300
    },
    {
      "epoch": 2.4717430101130278,
      "grad_norm": 2.130288600921631,
      "learning_rate": 3.523696212571882e-06,
      "loss": 0.9099,
      "step": 8310
    },
    {
      "epoch": 2.4747174301011303,
      "grad_norm": 6.369117259979248,
      "learning_rate": 3.503866745984533e-06,
      "loss": 0.9644,
      "step": 8320
    },
    {
      "epoch": 2.4776918500892324,
      "grad_norm": 3.286302089691162,
      "learning_rate": 3.4840372793971846e-06,
      "loss": 0.8771,
      "step": 8330
    },
    {
      "epoch": 2.480666270077335,
      "grad_norm": 2.107198715209961,
      "learning_rate": 3.4642078128098357e-06,
      "loss": 0.8344,
      "step": 8340
    },
    {
      "epoch": 2.483640690065437,
      "grad_norm": 2.0978472232818604,
      "learning_rate": 3.4443783462224868e-06,
      "loss": 0.8995,
      "step": 8350
    },
    {
      "epoch": 2.4866151100535396,
      "grad_norm": 2.817645788192749,
      "learning_rate": 3.424548879635138e-06,
      "loss": 1.0334,
      "step": 8360
    },
    {
      "epoch": 2.4895895300416417,
      "grad_norm": 2.323195219039917,
      "learning_rate": 3.4047194130477894e-06,
      "loss": 0.9481,
      "step": 8370
    },
    {
      "epoch": 2.4925639500297443,
      "grad_norm": 2.1923346519470215,
      "learning_rate": 3.3848899464604405e-06,
      "loss": 0.8899,
      "step": 8380
    },
    {
      "epoch": 2.4955383700178464,
      "grad_norm": 2.04573392868042,
      "learning_rate": 3.365060479873092e-06,
      "loss": 0.8664,
      "step": 8390
    },
    {
      "epoch": 2.498512790005949,
      "grad_norm": 2.037158250808716,
      "learning_rate": 3.345231013285743e-06,
      "loss": 0.8547,
      "step": 8400
    },
    {
      "epoch": 2.501487209994051,
      "grad_norm": 2.077629327774048,
      "learning_rate": 3.325401546698394e-06,
      "loss": 0.8479,
      "step": 8410
    },
    {
      "epoch": 2.5044616299821536,
      "grad_norm": 2.0679028034210205,
      "learning_rate": 3.3055720801110453e-06,
      "loss": 0.8062,
      "step": 8420
    },
    {
      "epoch": 2.5074360499702557,
      "grad_norm": 1.9515190124511719,
      "learning_rate": 3.2857426135236964e-06,
      "loss": 0.9586,
      "step": 8430
    },
    {
      "epoch": 2.5104104699583583,
      "grad_norm": 3.2515389919281006,
      "learning_rate": 3.2659131469363475e-06,
      "loss": 0.9902,
      "step": 8440
    },
    {
      "epoch": 2.5133848899464604,
      "grad_norm": 2.667367935180664,
      "learning_rate": 3.2460836803489986e-06,
      "loss": 0.9924,
      "step": 8450
    },
    {
      "epoch": 2.516359309934563,
      "grad_norm": 2.3735992908477783,
      "learning_rate": 3.2262542137616497e-06,
      "loss": 0.8608,
      "step": 8460
    },
    {
      "epoch": 2.519333729922665,
      "grad_norm": 23.892507553100586,
      "learning_rate": 3.2064247471743016e-06,
      "loss": 0.9684,
      "step": 8470
    },
    {
      "epoch": 2.522308149910767,
      "grad_norm": 1.7134873867034912,
      "learning_rate": 3.1865952805869527e-06,
      "loss": 0.7587,
      "step": 8480
    },
    {
      "epoch": 2.5252825698988697,
      "grad_norm": 2.010706663131714,
      "learning_rate": 3.1667658139996038e-06,
      "loss": 0.8815,
      "step": 8490
    },
    {
      "epoch": 2.5282569898869722,
      "grad_norm": 2.1879446506500244,
      "learning_rate": 3.146936347412255e-06,
      "loss": 0.781,
      "step": 8500
    },
    {
      "epoch": 2.5312314098750743,
      "grad_norm": 1.6496903896331787,
      "learning_rate": 3.127106880824906e-06,
      "loss": 0.8134,
      "step": 8510
    },
    {
      "epoch": 2.5342058298631764,
      "grad_norm": 2.426408290863037,
      "learning_rate": 3.107277414237557e-06,
      "loss": 0.9121,
      "step": 8520
    },
    {
      "epoch": 2.537180249851279,
      "grad_norm": 1.926572322845459,
      "learning_rate": 3.0874479476502086e-06,
      "loss": 0.8969,
      "step": 8530
    },
    {
      "epoch": 2.5401546698393815,
      "grad_norm": 1.4552339315414429,
      "learning_rate": 3.0676184810628597e-06,
      "loss": 1.0013,
      "step": 8540
    },
    {
      "epoch": 2.5431290898274836,
      "grad_norm": 2.240476131439209,
      "learning_rate": 3.0477890144755108e-06,
      "loss": 0.8969,
      "step": 8550
    },
    {
      "epoch": 2.5461035098155858,
      "grad_norm": 2.2574033737182617,
      "learning_rate": 3.027959547888162e-06,
      "loss": 0.9691,
      "step": 8560
    },
    {
      "epoch": 2.5490779298036883,
      "grad_norm": 2.111534595489502,
      "learning_rate": 3.0081300813008134e-06,
      "loss": 0.9059,
      "step": 8570
    },
    {
      "epoch": 2.552052349791791,
      "grad_norm": 1.6819888353347778,
      "learning_rate": 2.9883006147134645e-06,
      "loss": 0.8276,
      "step": 8580
    },
    {
      "epoch": 2.555026769779893,
      "grad_norm": 1.4052904844284058,
      "learning_rate": 2.968471148126116e-06,
      "loss": 0.882,
      "step": 8590
    },
    {
      "epoch": 2.558001189767995,
      "grad_norm": 2.386530876159668,
      "learning_rate": 2.948641681538767e-06,
      "loss": 0.8958,
      "step": 8600
    },
    {
      "epoch": 2.5609756097560976,
      "grad_norm": 2.081894874572754,
      "learning_rate": 2.928812214951418e-06,
      "loss": 0.8504,
      "step": 8610
    },
    {
      "epoch": 2.5639500297441997,
      "grad_norm": 2.075800657272339,
      "learning_rate": 2.9089827483640693e-06,
      "loss": 0.7615,
      "step": 8620
    },
    {
      "epoch": 2.5669244497323023,
      "grad_norm": 1.871657371520996,
      "learning_rate": 2.8891532817767204e-06,
      "loss": 0.8344,
      "step": 8630
    },
    {
      "epoch": 2.5698988697204044,
      "grad_norm": 3.1298065185546875,
      "learning_rate": 2.8693238151893715e-06,
      "loss": 0.8134,
      "step": 8640
    },
    {
      "epoch": 2.572873289708507,
      "grad_norm": 2.125675678253174,
      "learning_rate": 2.8494943486020226e-06,
      "loss": 0.9076,
      "step": 8650
    },
    {
      "epoch": 2.575847709696609,
      "grad_norm": 2.635277271270752,
      "learning_rate": 2.8296648820146737e-06,
      "loss": 0.9395,
      "step": 8660
    },
    {
      "epoch": 2.5788221296847116,
      "grad_norm": 2.709653377532959,
      "learning_rate": 2.809835415427325e-06,
      "loss": 0.8886,
      "step": 8670
    },
    {
      "epoch": 2.5817965496728137,
      "grad_norm": 2.336470127105713,
      "learning_rate": 2.7900059488399767e-06,
      "loss": 0.9886,
      "step": 8680
    },
    {
      "epoch": 2.5847709696609162,
      "grad_norm": 2.132646322250366,
      "learning_rate": 2.7701764822526278e-06,
      "loss": 0.8858,
      "step": 8690
    },
    {
      "epoch": 2.5877453896490183,
      "grad_norm": 2.6583364009857178,
      "learning_rate": 2.750347015665279e-06,
      "loss": 0.8875,
      "step": 8700
    },
    {
      "epoch": 2.590719809637121,
      "grad_norm": 3.2419636249542236,
      "learning_rate": 2.73051754907793e-06,
      "loss": 0.8417,
      "step": 8710
    },
    {
      "epoch": 2.593694229625223,
      "grad_norm": 3.9302613735198975,
      "learning_rate": 2.710688082490581e-06,
      "loss": 0.9094,
      "step": 8720
    },
    {
      "epoch": 2.5966686496133256,
      "grad_norm": 2.7943344116210938,
      "learning_rate": 2.6908586159032326e-06,
      "loss": 0.8451,
      "step": 8730
    },
    {
      "epoch": 2.5996430696014277,
      "grad_norm": 2.3445065021514893,
      "learning_rate": 2.6710291493158837e-06,
      "loss": 0.968,
      "step": 8740
    },
    {
      "epoch": 2.60261748958953,
      "grad_norm": 1.8330055475234985,
      "learning_rate": 2.6511996827285348e-06,
      "loss": 0.8459,
      "step": 8750
    },
    {
      "epoch": 2.6055919095776323,
      "grad_norm": 2.904496431350708,
      "learning_rate": 2.631370216141186e-06,
      "loss": 0.7962,
      "step": 8760
    },
    {
      "epoch": 2.6085663295657344,
      "grad_norm": 1.874333143234253,
      "learning_rate": 2.611540749553837e-06,
      "loss": 0.8841,
      "step": 8770
    },
    {
      "epoch": 2.611540749553837,
      "grad_norm": 2.2366137504577637,
      "learning_rate": 2.5917112829664885e-06,
      "loss": 0.894,
      "step": 8780
    },
    {
      "epoch": 2.6145151695419395,
      "grad_norm": 2.0249786376953125,
      "learning_rate": 2.57188181637914e-06,
      "loss": 0.9025,
      "step": 8790
    },
    {
      "epoch": 2.6174895895300416,
      "grad_norm": 2.271886110305786,
      "learning_rate": 2.552052349791791e-06,
      "loss": 0.916,
      "step": 8800
    },
    {
      "epoch": 2.6204640095181437,
      "grad_norm": 1.8045028448104858,
      "learning_rate": 2.532222883204442e-06,
      "loss": 0.9073,
      "step": 8810
    },
    {
      "epoch": 2.6234384295062463,
      "grad_norm": 1.8432567119598389,
      "learning_rate": 2.5123934166170933e-06,
      "loss": 0.8608,
      "step": 8820
    },
    {
      "epoch": 2.626412849494349,
      "grad_norm": 3.8072667121887207,
      "learning_rate": 2.4925639500297444e-06,
      "loss": 0.9486,
      "step": 8830
    },
    {
      "epoch": 2.629387269482451,
      "grad_norm": 1.8289480209350586,
      "learning_rate": 2.4727344834423955e-06,
      "loss": 0.7798,
      "step": 8840
    },
    {
      "epoch": 2.632361689470553,
      "grad_norm": 1.6013132333755493,
      "learning_rate": 2.4529050168550466e-06,
      "loss": 0.8115,
      "step": 8850
    },
    {
      "epoch": 2.6353361094586556,
      "grad_norm": 1.694329857826233,
      "learning_rate": 2.433075550267698e-06,
      "loss": 0.8568,
      "step": 8860
    },
    {
      "epoch": 2.638310529446758,
      "grad_norm": 2.163856029510498,
      "learning_rate": 2.413246083680349e-06,
      "loss": 0.8763,
      "step": 8870
    },
    {
      "epoch": 2.6412849494348603,
      "grad_norm": 5.660820484161377,
      "learning_rate": 2.3934166170930003e-06,
      "loss": 0.8191,
      "step": 8880
    },
    {
      "epoch": 2.6442593694229624,
      "grad_norm": 3.1718544960021973,
      "learning_rate": 2.3735871505056514e-06,
      "loss": 0.8558,
      "step": 8890
    },
    {
      "epoch": 2.647233789411065,
      "grad_norm": 2.3885107040405273,
      "learning_rate": 2.353757683918303e-06,
      "loss": 1.1408,
      "step": 8900
    },
    {
      "epoch": 2.650208209399167,
      "grad_norm": 1.775758981704712,
      "learning_rate": 2.333928217330954e-06,
      "loss": 0.9176,
      "step": 8910
    },
    {
      "epoch": 2.6531826293872696,
      "grad_norm": 2.120882034301758,
      "learning_rate": 2.314098750743605e-06,
      "loss": 0.8994,
      "step": 8920
    },
    {
      "epoch": 2.6561570493753717,
      "grad_norm": 1.7939269542694092,
      "learning_rate": 2.2942692841562566e-06,
      "loss": 0.7979,
      "step": 8930
    },
    {
      "epoch": 2.659131469363474,
      "grad_norm": 2.0530056953430176,
      "learning_rate": 2.2744398175689077e-06,
      "loss": 0.8545,
      "step": 8940
    },
    {
      "epoch": 2.6621058893515763,
      "grad_norm": 1.9608501195907593,
      "learning_rate": 2.2546103509815588e-06,
      "loss": 0.7725,
      "step": 8950
    },
    {
      "epoch": 2.665080309339679,
      "grad_norm": 2.5363831520080566,
      "learning_rate": 2.2347808843942103e-06,
      "loss": 0.9553,
      "step": 8960
    },
    {
      "epoch": 2.668054729327781,
      "grad_norm": 2.281745433807373,
      "learning_rate": 2.2149514178068614e-06,
      "loss": 0.8358,
      "step": 8970
    },
    {
      "epoch": 2.6710291493158835,
      "grad_norm": 4.583335876464844,
      "learning_rate": 2.1951219512195125e-06,
      "loss": 0.81,
      "step": 8980
    },
    {
      "epoch": 2.6740035693039856,
      "grad_norm": 2.009371519088745,
      "learning_rate": 2.1752924846321636e-06,
      "loss": 0.8015,
      "step": 8990
    },
    {
      "epoch": 2.676977989292088,
      "grad_norm": 2.0311567783355713,
      "learning_rate": 2.1554630180448147e-06,
      "loss": 0.9122,
      "step": 9000
    },
    {
      "epoch": 2.6799524092801903,
      "grad_norm": 4.6178789138793945,
      "learning_rate": 2.135633551457466e-06,
      "loss": 0.7263,
      "step": 9010
    },
    {
      "epoch": 2.682926829268293,
      "grad_norm": 1.8999686241149902,
      "learning_rate": 2.1158040848701173e-06,
      "loss": 0.9082,
      "step": 9020
    },
    {
      "epoch": 2.685901249256395,
      "grad_norm": 1.9960548877716064,
      "learning_rate": 2.0959746182827684e-06,
      "loss": 0.8799,
      "step": 9030
    },
    {
      "epoch": 2.6888756692444975,
      "grad_norm": 2.1434497833251953,
      "learning_rate": 2.0761451516954195e-06,
      "loss": 0.9007,
      "step": 9040
    },
    {
      "epoch": 2.6918500892325996,
      "grad_norm": 2.5565998554229736,
      "learning_rate": 2.0563156851080705e-06,
      "loss": 0.9227,
      "step": 9050
    },
    {
      "epoch": 2.6948245092207017,
      "grad_norm": 2.092439651489258,
      "learning_rate": 2.036486218520722e-06,
      "loss": 0.8611,
      "step": 9060
    },
    {
      "epoch": 2.6977989292088043,
      "grad_norm": 1.402884602546692,
      "learning_rate": 2.016656751933373e-06,
      "loss": 0.7613,
      "step": 9070
    },
    {
      "epoch": 2.700773349196907,
      "grad_norm": 2.1443135738372803,
      "learning_rate": 1.9968272853460243e-06,
      "loss": 1.0854,
      "step": 9080
    },
    {
      "epoch": 2.703747769185009,
      "grad_norm": 2.22430682182312,
      "learning_rate": 1.9769978187586753e-06,
      "loss": 0.9989,
      "step": 9090
    },
    {
      "epoch": 2.706722189173111,
      "grad_norm": 1.9938212633132935,
      "learning_rate": 1.957168352171327e-06,
      "loss": 0.8234,
      "step": 9100
    },
    {
      "epoch": 2.7096966091612136,
      "grad_norm": 3.4799084663391113,
      "learning_rate": 1.937338885583978e-06,
      "loss": 0.9078,
      "step": 9110
    },
    {
      "epoch": 2.712671029149316,
      "grad_norm": 1.760756015777588,
      "learning_rate": 1.917509418996629e-06,
      "loss": 0.9217,
      "step": 9120
    },
    {
      "epoch": 2.7156454491374182,
      "grad_norm": 1.8753831386566162,
      "learning_rate": 1.8976799524092804e-06,
      "loss": 0.9915,
      "step": 9130
    },
    {
      "epoch": 2.7186198691255203,
      "grad_norm": 2.220200538635254,
      "learning_rate": 1.8778504858219315e-06,
      "loss": 0.8232,
      "step": 9140
    },
    {
      "epoch": 2.721594289113623,
      "grad_norm": 2.0426833629608154,
      "learning_rate": 1.8580210192345828e-06,
      "loss": 0.8987,
      "step": 9150
    },
    {
      "epoch": 2.7245687091017254,
      "grad_norm": 1.8212454319000244,
      "learning_rate": 1.8381915526472339e-06,
      "loss": 0.8118,
      "step": 9160
    },
    {
      "epoch": 2.7275431290898275,
      "grad_norm": 2.1195085048675537,
      "learning_rate": 1.8183620860598852e-06,
      "loss": 0.889,
      "step": 9170
    },
    {
      "epoch": 2.7305175490779297,
      "grad_norm": 2.24316668510437,
      "learning_rate": 1.7985326194725365e-06,
      "loss": 0.9094,
      "step": 9180
    },
    {
      "epoch": 2.733491969066032,
      "grad_norm": 1.6952983140945435,
      "learning_rate": 1.7787031528851876e-06,
      "loss": 0.8415,
      "step": 9190
    },
    {
      "epoch": 2.7364663890541343,
      "grad_norm": 1.6977670192718506,
      "learning_rate": 1.7588736862978387e-06,
      "loss": 0.8109,
      "step": 9200
    },
    {
      "epoch": 2.739440809042237,
      "grad_norm": 5.196549892425537,
      "learning_rate": 1.7390442197104897e-06,
      "loss": 0.9014,
      "step": 9210
    },
    {
      "epoch": 2.742415229030339,
      "grad_norm": 2.3085217475891113,
      "learning_rate": 1.7192147531231413e-06,
      "loss": 0.9199,
      "step": 9220
    },
    {
      "epoch": 2.7453896490184415,
      "grad_norm": 2.550368547439575,
      "learning_rate": 1.6993852865357924e-06,
      "loss": 0.9307,
      "step": 9230
    },
    {
      "epoch": 2.7483640690065436,
      "grad_norm": 1.9476234912872314,
      "learning_rate": 1.6795558199484435e-06,
      "loss": 0.8051,
      "step": 9240
    },
    {
      "epoch": 2.751338488994646,
      "grad_norm": 1.7960928678512573,
      "learning_rate": 1.6597263533610948e-06,
      "loss": 0.9257,
      "step": 9250
    },
    {
      "epoch": 2.7543129089827483,
      "grad_norm": 2.3850836753845215,
      "learning_rate": 1.6398968867737459e-06,
      "loss": 0.8948,
      "step": 9260
    },
    {
      "epoch": 2.757287328970851,
      "grad_norm": 1.9058058261871338,
      "learning_rate": 1.6200674201863972e-06,
      "loss": 1.0267,
      "step": 9270
    },
    {
      "epoch": 2.760261748958953,
      "grad_norm": 1.8241864442825317,
      "learning_rate": 1.6002379535990485e-06,
      "loss": 0.9305,
      "step": 9280
    },
    {
      "epoch": 2.7632361689470555,
      "grad_norm": 1.91947603225708,
      "learning_rate": 1.5804084870116996e-06,
      "loss": 0.8245,
      "step": 9290
    },
    {
      "epoch": 2.7662105889351576,
      "grad_norm": 2.826395273208618,
      "learning_rate": 1.5605790204243506e-06,
      "loss": 0.8981,
      "step": 9300
    },
    {
      "epoch": 2.7691850089232597,
      "grad_norm": 1.7391537427902222,
      "learning_rate": 1.5407495538370017e-06,
      "loss": 0.815,
      "step": 9310
    },
    {
      "epoch": 2.7721594289113622,
      "grad_norm": 1.8345705270767212,
      "learning_rate": 1.5209200872496533e-06,
      "loss": 0.8052,
      "step": 9320
    },
    {
      "epoch": 2.775133848899465,
      "grad_norm": 1.8138478994369507,
      "learning_rate": 1.5010906206623044e-06,
      "loss": 0.8861,
      "step": 9330
    },
    {
      "epoch": 2.778108268887567,
      "grad_norm": 2.2108263969421387,
      "learning_rate": 1.4812611540749554e-06,
      "loss": 0.7943,
      "step": 9340
    },
    {
      "epoch": 2.781082688875669,
      "grad_norm": 1.8470290899276733,
      "learning_rate": 1.4614316874876068e-06,
      "loss": 0.9778,
      "step": 9350
    },
    {
      "epoch": 2.7840571088637716,
      "grad_norm": 2.933640718460083,
      "learning_rate": 1.4416022209002578e-06,
      "loss": 0.9271,
      "step": 9360
    },
    {
      "epoch": 2.787031528851874,
      "grad_norm": 2.3633270263671875,
      "learning_rate": 1.4217727543129092e-06,
      "loss": 0.9846,
      "step": 9370
    },
    {
      "epoch": 2.790005948839976,
      "grad_norm": 1.6983615159988403,
      "learning_rate": 1.4019432877255605e-06,
      "loss": 0.8962,
      "step": 9380
    },
    {
      "epoch": 2.7929803688280783,
      "grad_norm": 2.0914089679718018,
      "learning_rate": 1.3821138211382116e-06,
      "loss": 0.9037,
      "step": 9390
    },
    {
      "epoch": 2.795954788816181,
      "grad_norm": 1.6407384872436523,
      "learning_rate": 1.3622843545508626e-06,
      "loss": 0.9678,
      "step": 9400
    },
    {
      "epoch": 2.7989292088042834,
      "grad_norm": 2.1584572792053223,
      "learning_rate": 1.3424548879635137e-06,
      "loss": 0.8127,
      "step": 9410
    },
    {
      "epoch": 2.8019036287923855,
      "grad_norm": 1.7090446949005127,
      "learning_rate": 1.322625421376165e-06,
      "loss": 0.8899,
      "step": 9420
    },
    {
      "epoch": 2.8048780487804876,
      "grad_norm": 2.128657817840576,
      "learning_rate": 1.3027959547888164e-06,
      "loss": 0.9371,
      "step": 9430
    },
    {
      "epoch": 2.80785246876859,
      "grad_norm": 2.3077285289764404,
      "learning_rate": 1.2829664882014674e-06,
      "loss": 0.8693,
      "step": 9440
    },
    {
      "epoch": 2.8108268887566923,
      "grad_norm": 2.650416851043701,
      "learning_rate": 1.2631370216141188e-06,
      "loss": 0.8623,
      "step": 9450
    },
    {
      "epoch": 2.813801308744795,
      "grad_norm": 2.1836585998535156,
      "learning_rate": 1.2433075550267698e-06,
      "loss": 1.0129,
      "step": 9460
    },
    {
      "epoch": 2.816775728732897,
      "grad_norm": 1.9269546270370483,
      "learning_rate": 1.2234780884394212e-06,
      "loss": 0.824,
      "step": 9470
    },
    {
      "epoch": 2.8197501487209995,
      "grad_norm": 2.2324740886688232,
      "learning_rate": 1.2036486218520722e-06,
      "loss": 0.871,
      "step": 9480
    },
    {
      "epoch": 2.8227245687091016,
      "grad_norm": 1.6826921701431274,
      "learning_rate": 1.1838191552647236e-06,
      "loss": 0.8882,
      "step": 9490
    },
    {
      "epoch": 2.825698988697204,
      "grad_norm": 1.358415126800537,
      "learning_rate": 1.1639896886773746e-06,
      "loss": 0.8009,
      "step": 9500
    },
    {
      "epoch": 2.8286734086853063,
      "grad_norm": 1.5554877519607544,
      "learning_rate": 1.1441602220900257e-06,
      "loss": 0.9195,
      "step": 9510
    },
    {
      "epoch": 2.831647828673409,
      "grad_norm": 2.22052264213562,
      "learning_rate": 1.124330755502677e-06,
      "loss": 0.8607,
      "step": 9520
    },
    {
      "epoch": 2.834622248661511,
      "grad_norm": 2.416938304901123,
      "learning_rate": 1.1045012889153281e-06,
      "loss": 0.8704,
      "step": 9530
    },
    {
      "epoch": 2.8375966686496135,
      "grad_norm": 2.4162776470184326,
      "learning_rate": 1.0846718223279794e-06,
      "loss": 0.896,
      "step": 9540
    },
    {
      "epoch": 2.8405710886377156,
      "grad_norm": 2.218747854232788,
      "learning_rate": 1.0648423557406308e-06,
      "loss": 1.007,
      "step": 9550
    },
    {
      "epoch": 2.843545508625818,
      "grad_norm": 1.7569458484649658,
      "learning_rate": 1.0450128891532818e-06,
      "loss": 0.8119,
      "step": 9560
    },
    {
      "epoch": 2.8465199286139202,
      "grad_norm": 1.8219538927078247,
      "learning_rate": 1.0251834225659332e-06,
      "loss": 0.9349,
      "step": 9570
    },
    {
      "epoch": 2.8494943486020228,
      "grad_norm": 2.4994924068450928,
      "learning_rate": 1.0053539559785842e-06,
      "loss": 0.9422,
      "step": 9580
    },
    {
      "epoch": 2.852468768590125,
      "grad_norm": 2.1604485511779785,
      "learning_rate": 9.855244893912355e-07,
      "loss": 0.8884,
      "step": 9590
    },
    {
      "epoch": 2.855443188578227,
      "grad_norm": 1.5950911045074463,
      "learning_rate": 9.656950228038866e-07,
      "loss": 0.8078,
      "step": 9600
    },
    {
      "epoch": 2.8584176085663295,
      "grad_norm": 1.9339993000030518,
      "learning_rate": 9.458655562165378e-07,
      "loss": 0.7798,
      "step": 9610
    },
    {
      "epoch": 2.861392028554432,
      "grad_norm": 2.0562903881073,
      "learning_rate": 9.26036089629189e-07,
      "loss": 0.8905,
      "step": 9620
    },
    {
      "epoch": 2.864366448542534,
      "grad_norm": 1.6978106498718262,
      "learning_rate": 9.062066230418402e-07,
      "loss": 0.9105,
      "step": 9630
    },
    {
      "epoch": 2.8673408685306363,
      "grad_norm": 2.004176139831543,
      "learning_rate": 8.863771564544913e-07,
      "loss": 0.9009,
      "step": 9640
    },
    {
      "epoch": 2.870315288518739,
      "grad_norm": 1.8014755249023438,
      "learning_rate": 8.665476898671426e-07,
      "loss": 0.8656,
      "step": 9650
    },
    {
      "epoch": 2.8732897085068414,
      "grad_norm": 5.168188571929932,
      "learning_rate": 8.467182232797938e-07,
      "loss": 0.8222,
      "step": 9660
    },
    {
      "epoch": 2.8762641284949435,
      "grad_norm": 2.008629560470581,
      "learning_rate": 8.26888756692445e-07,
      "loss": 0.8275,
      "step": 9670
    },
    {
      "epoch": 2.8792385484830456,
      "grad_norm": 2.3576135635375977,
      "learning_rate": 8.070592901050962e-07,
      "loss": 1.0073,
      "step": 9680
    },
    {
      "epoch": 2.882212968471148,
      "grad_norm": 1.7384930849075317,
      "learning_rate": 7.872298235177473e-07,
      "loss": 0.92,
      "step": 9690
    },
    {
      "epoch": 2.8851873884592507,
      "grad_norm": 2.0731377601623535,
      "learning_rate": 7.674003569303986e-07,
      "loss": 0.8988,
      "step": 9700
    },
    {
      "epoch": 2.888161808447353,
      "grad_norm": 3.1362457275390625,
      "learning_rate": 7.475708903430498e-07,
      "loss": 0.8792,
      "step": 9710
    },
    {
      "epoch": 2.891136228435455,
      "grad_norm": 2.975449323654175,
      "learning_rate": 7.27741423755701e-07,
      "loss": 0.8752,
      "step": 9720
    },
    {
      "epoch": 2.8941106484235575,
      "grad_norm": 1.9134809970855713,
      "learning_rate": 7.079119571683522e-07,
      "loss": 0.7685,
      "step": 9730
    },
    {
      "epoch": 2.8970850684116596,
      "grad_norm": 5.243871688842773,
      "learning_rate": 6.880824905810033e-07,
      "loss": 0.8244,
      "step": 9740
    },
    {
      "epoch": 2.900059488399762,
      "grad_norm": 3.2562270164489746,
      "learning_rate": 6.682530239936546e-07,
      "loss": 0.9842,
      "step": 9750
    },
    {
      "epoch": 2.9030339083878642,
      "grad_norm": 1.8330315351486206,
      "learning_rate": 6.484235574063058e-07,
      "loss": 0.7931,
      "step": 9760
    },
    {
      "epoch": 2.906008328375967,
      "grad_norm": 2.665799856185913,
      "learning_rate": 6.28594090818957e-07,
      "loss": 0.9539,
      "step": 9770
    },
    {
      "epoch": 2.908982748364069,
      "grad_norm": 2.0294406414031982,
      "learning_rate": 6.087646242316082e-07,
      "loss": 0.8756,
      "step": 9780
    },
    {
      "epoch": 2.9119571683521714,
      "grad_norm": 2.0329411029815674,
      "learning_rate": 5.889351576442594e-07,
      "loss": 0.9262,
      "step": 9790
    },
    {
      "epoch": 2.9149315883402735,
      "grad_norm": 2.274566411972046,
      "learning_rate": 5.691056910569106e-07,
      "loss": 0.9471,
      "step": 9800
    },
    {
      "epoch": 2.917906008328376,
      "grad_norm": 2.2717936038970947,
      "learning_rate": 5.492762244695618e-07,
      "loss": 0.8414,
      "step": 9810
    },
    {
      "epoch": 2.920880428316478,
      "grad_norm": 1.9822585582733154,
      "learning_rate": 5.29446757882213e-07,
      "loss": 0.955,
      "step": 9820
    },
    {
      "epoch": 2.9238548483045808,
      "grad_norm": 2.0220155715942383,
      "learning_rate": 5.096172912948642e-07,
      "loss": 0.8653,
      "step": 9830
    },
    {
      "epoch": 2.926829268292683,
      "grad_norm": 2.2180211544036865,
      "learning_rate": 4.897878247075153e-07,
      "loss": 0.9059,
      "step": 9840
    },
    {
      "epoch": 2.9298036882807854,
      "grad_norm": 3.4154841899871826,
      "learning_rate": 4.699583581201666e-07,
      "loss": 0.9136,
      "step": 9850
    },
    {
      "epoch": 2.9327781082688875,
      "grad_norm": 1.9579973220825195,
      "learning_rate": 4.501288915328178e-07,
      "loss": 0.924,
      "step": 9860
    },
    {
      "epoch": 2.93575252825699,
      "grad_norm": 1.9338616132736206,
      "learning_rate": 4.3029942494546904e-07,
      "loss": 0.8172,
      "step": 9870
    },
    {
      "epoch": 2.938726948245092,
      "grad_norm": 1.889404058456421,
      "learning_rate": 4.1046995835812024e-07,
      "loss": 0.7528,
      "step": 9880
    },
    {
      "epoch": 2.9417013682331943,
      "grad_norm": 1.726313591003418,
      "learning_rate": 3.906404917707714e-07,
      "loss": 0.7021,
      "step": 9890
    },
    {
      "epoch": 2.944675788221297,
      "grad_norm": 2.4389586448669434,
      "learning_rate": 3.708110251834226e-07,
      "loss": 0.8145,
      "step": 9900
    },
    {
      "epoch": 2.9476502082093994,
      "grad_norm": 3.2279632091522217,
      "learning_rate": 3.509815585960738e-07,
      "loss": 0.9407,
      "step": 9910
    },
    {
      "epoch": 2.9506246281975015,
      "grad_norm": 3.440708875656128,
      "learning_rate": 3.3115209200872504e-07,
      "loss": 0.9391,
      "step": 9920
    },
    {
      "epoch": 2.9535990481856036,
      "grad_norm": 2.1633222103118896,
      "learning_rate": 3.113226254213762e-07,
      "loss": 1.0762,
      "step": 9930
    },
    {
      "epoch": 2.956573468173706,
      "grad_norm": 20.20213508605957,
      "learning_rate": 2.914931588340274e-07,
      "loss": 0.8633,
      "step": 9940
    },
    {
      "epoch": 2.9595478881618087,
      "grad_norm": 1.7973029613494873,
      "learning_rate": 2.716636922466786e-07,
      "loss": 0.9306,
      "step": 9950
    },
    {
      "epoch": 2.962522308149911,
      "grad_norm": 1.624393105506897,
      "learning_rate": 2.518342256593298e-07,
      "loss": 0.8414,
      "step": 9960
    },
    {
      "epoch": 2.965496728138013,
      "grad_norm": 1.701969027519226,
      "learning_rate": 2.3200475907198098e-07,
      "loss": 0.8295,
      "step": 9970
    },
    {
      "epoch": 2.9684711481261155,
      "grad_norm": 2.131009817123413,
      "learning_rate": 2.1217529248463218e-07,
      "loss": 0.9844,
      "step": 9980
    },
    {
      "epoch": 2.9714455681142176,
      "grad_norm": 2.5333476066589355,
      "learning_rate": 1.9234582589728338e-07,
      "loss": 0.8813,
      "step": 9990
    },
    {
      "epoch": 2.97441998810232,
      "grad_norm": 2.1261978149414062,
      "learning_rate": 1.7251635930993458e-07,
      "loss": 0.8558,
      "step": 10000
    },
    {
      "epoch": 2.977394408090422,
      "grad_norm": 2.409710168838501,
      "learning_rate": 1.5268689272258578e-07,
      "loss": 0.8835,
      "step": 10010
    },
    {
      "epoch": 2.9803688280785248,
      "grad_norm": 1.8007084131240845,
      "learning_rate": 1.3285742613523698e-07,
      "loss": 0.9202,
      "step": 10020
    },
    {
      "epoch": 2.983343248066627,
      "grad_norm": 1.8640210628509521,
      "learning_rate": 1.1302795954788817e-07,
      "loss": 0.9464,
      "step": 10030
    },
    {
      "epoch": 2.9863176680547294,
      "grad_norm": 1.7628995180130005,
      "learning_rate": 9.319849296053937e-08,
      "loss": 0.7871,
      "step": 10040
    },
    {
      "epoch": 2.9892920880428315,
      "grad_norm": 2.195220470428467,
      "learning_rate": 7.336902637319057e-08,
      "loss": 0.9163,
      "step": 10050
    },
    {
      "epoch": 2.992266508030934,
      "grad_norm": 3.9740095138549805,
      "learning_rate": 5.353955978584176e-08,
      "loss": 0.9416,
      "step": 10060
    },
    {
      "epoch": 2.995240928019036,
      "grad_norm": 2.224217414855957,
      "learning_rate": 3.371009319849297e-08,
      "loss": 0.9875,
      "step": 10070
    },
    {
      "epoch": 2.9982153480071387,
      "grad_norm": 8.873218536376953,
      "learning_rate": 1.3880626611144161e-08,
      "loss": 0.8884,
      "step": 10080
    }
  ],
  "logging_steps": 10,
  "max_steps": 10086,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1364854396944384.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
